{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet1d GRU Train - 1 / 5 Dataset\n",
    "\n",
    "- https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-train-1-5-dataset/notebook\n",
    "    - CV: 0.3064\n",
    "    - LB: 0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールの動的import(import先のファイルが更新されたときに追従する)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubuntu 20.04.6 LTS\n",
      "BUILD_DATE=20240318-171459, CONTAINER_NAME=tf2-gpu/2-15+cu121\n",
      "PyTorch Version:2.0.1+cu117, CUDA is available:True, Version CUDA:11.7\n",
      "Device Capability:(8, 9), ['sm_37', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86']\n",
      "CuDNN Enabled:True, Version:8500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    ")\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from src.metrics import kl_div\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "! cat /etc/os-release | grep -oP \"PRETTY_NAME=\\\"\\K([^\\\"]*)\"\n",
    "print(f\"BUILD_DATE={os.environ['BUILD_DATE']}, CONTAINER_NAME={os.environ['CONTAINER_NAME']}\")\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"PyTorch Version:{torch.__version__}, CUDA is available:{torch.cuda.is_available()}, Version CUDA:{torch.version.cuda}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Device Capability:{torch.cuda.get_device_capability()}, {torch.cuda.get_arch_list()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"CuDNN Enabled:{torch.backends.cudnn.enabled}, Version:{torch.backends.cudnn.version()}\"\n",
    "    )\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/kaggle/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cfg.v3 import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encoding(data, classes)\n",
    "    return mu_x  # quantized\n",
    "\n",
    "\n",
    "def mu_law_encoding(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "\n",
    "def mu_law_expansion(data, mu):\n",
    "    s = np.sign(data) * (np.exp(np.abs(data) * np.log(mu + 1)) - 1) / mu\n",
    "    return s\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype=\"band\")\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(\n",
    "    data, cutoff_freq=20, sampling_rate=CFG.sampling_rate, order=4\n",
    "):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
    "    filtered_data = lfilter(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def denoise_filter(x):\n",
    "    y = butter_bandpass_filter(x, CFG.lowcut, CFG.highcut, CFG.sampling_rate, order=6)\n",
    "    y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
    "    y = y[0:-1:4]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_from_parquet(\n",
    "    parquet_path: str, display: bool = False, seq_length=CFG.seq_length\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Эта функция читает файл паркета и извлекает средние 50 секунд показаний. Затем он заполняет значения NaN\n",
    "    со средним значением (игнорируя NaN).\n",
    "        :param parquet_path: путь к файлу паркета.\n",
    "        :param display: отображать графики ЭЭГ или нет.\n",
    "        :return data: np.array формы (time_steps, eeg_features) -> (10_000, 8)\n",
    "    \"\"\"\n",
    "\n",
    "    # Вырезаем среднюю 50 секундную часть\n",
    "    eeg = pd.read_parquet(parquet_path, columns=CFG.eeg_features)\n",
    "    rows = len(eeg)\n",
    "\n",
    "    # начало смещения данных, чтобы забрать середину\n",
    "    offset = (rows - CFG.nsamples) // 2\n",
    "\n",
    "    # средние 50 секунд, имеет одинаковое количество показаний слева и справа\n",
    "    eeg = eeg.iloc[offset : offset + CFG.nsamples]\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        offset = 0\n",
    "\n",
    "    # Конвертировать в numpy\n",
    "\n",
    "    # создать заполнитель той же формы с нулями\n",
    "    data = np.zeros((CFG.nsamples, len(CFG.eeg_features)))\n",
    "\n",
    "    for index, feature in enumerate(CFG.eeg_features):\n",
    "        x = eeg[feature].values.astype(\"float32\")  # конвертировать в float32\n",
    "\n",
    "        # Вычисляет среднее арифметическое вдоль указанной оси, игнорируя NaN.\n",
    "        mean = np.nanmean(x)\n",
    "        nan_percentage = np.isnan(x).mean()  # percentage of NaN values in feature\n",
    "\n",
    "        # Заполнение значения Nan\n",
    "        # Поэлементная проверка на NaN и возврат результата в виде логического массива.\n",
    "        if nan_percentage < 1:  # если некоторые значения равны Nan, но не все\n",
    "            x = np.nan_to_num(x, nan=mean)\n",
    "        else:  # если все значения — Nan\n",
    "            x[:] = 0\n",
    "        data[:, index] = x\n",
    "\n",
    "        if display:\n",
    "            if index != 0:\n",
    "                offset += x.max()\n",
    "            plt.plot(range(CFG.nsamples), x - offset, label=feature)\n",
    "            offset -= x.min()\n",
    "\n",
    "    if display:\n",
    "        plt.legend()\n",
    "        name = parquet_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"EEG {name}\", size=16)\n",
    "        plt.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        eegs: Dict[int, np.ndarray],\n",
    "        mode: str = \"train\",\n",
    "        downsample: int = None,\n",
    "        bandpass_filter: Dict[str, Union[int, float]] = None,\n",
    "        rand_filter: Dict[str, Union[int, float]] = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.eegs = eegs\n",
    "        self.downsample = downsample\n",
    "        self.offset = None\n",
    "        self.bandpass_filter = bandpass_filter\n",
    "        self.rand_filter = rand_filter\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length of dataset.\n",
    "        \"\"\"\n",
    "        # Обозначает количество пакетов за эпоху\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get one item.\n",
    "        \"\"\"\n",
    "        # Сгенерировать один пакет данных\n",
    "        X, y_prob = self.__data_generation(index)\n",
    "        if self.downsample is not None:\n",
    "            X = X[:: self.downsample, :]\n",
    "        output = {\n",
    "            \"eeg\": torch.tensor(X, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(y_prob, dtype=torch.float32),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def set_offset(self, offset: int):\n",
    "        self.offset = offset\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        # Генерирует данные, содержащие образцы размера партии\n",
    "        X = np.zeros(\n",
    "            (CFG.out_samples, CFG.in_channels), dtype=\"float32\"\n",
    "        )  # Size=(10000, 14)\n",
    "\n",
    "        random_divide_signal = False\n",
    "        row = self.df.iloc[index]  # Строка Pandas\n",
    "        data = self.eegs[row.eeg_id]  # Size=(10000, 8)\n",
    "        if CFG.nsamples != CFG.out_samples:\n",
    "            if self.mode == \"train\":\n",
    "                offset = (CFG.sample_delta * random.randint(0, 1000)) // 1000\n",
    "            elif not self.offset is None:\n",
    "                offset = self.offset\n",
    "            else:\n",
    "                offset = CFG.sample_offset\n",
    "\n",
    "            if self.mode == \"train\" and CFG.random_divide_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_divide_signal:\n",
    "                random_divide_signal = True\n",
    "                multipliers = [(1, 2), (2, 3), (3, 4), (3, 5)]\n",
    "                koef_1, koef_2 = multipliers[random.randint(0, 3)]\n",
    "                offset = (koef_1 * offset) // koef_2\n",
    "                data = data[offset:offset+(CFG.out_samples * koef_2) // koef_1,:]\n",
    "            else:\n",
    "                data = data[offset:offset+CFG.out_samples,:]\n",
    "\n",
    "        reverse_signal = False\n",
    "        negative_signal = False\n",
    "        if self.mode == \"train\":\n",
    "            if CFG.random_common_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_reverse_signal:\n",
    "                reverse_signal = True\n",
    "            if CFG.random_common_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_negative_signal:\n",
    "                negative_signal = True\n",
    "\n",
    "        for i, (feat_a, feat_b) in enumerate(CFG.map_features):\n",
    "            if self.mode == \"train\" and CFG.random_close_zone > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_close_zone:\n",
    "                continue\n",
    "\n",
    "            diff_feat = (\n",
    "                data[:, CFG.feature_to_index[feat_a]]\n",
    "                - data[:, CFG.feature_to_index[feat_b]]\n",
    "            )  # Size=(10000,)\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                if reverse_signal or CFG.random_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_reverse_signal:\n",
    "                    diff_feat = np.flip(diff_feat)\n",
    "                if negative_signal or CFG.random_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_negative_signal:\n",
    "                    diff_feat = -diff_feat\n",
    "\n",
    "            if not self.bandpass_filter is None:\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            if random_divide_signal:\n",
    "                #diff_feat = cp.asnumpy(cpsig.upfirdn([1.0, 1, 1.0], diff_feat, 2, 3))  # linear interp, rate 2/3\n",
    "                diff_feat = scisig.upfirdn([1.0, 1, 1.0], diff_feat, koef_1, koef_2)  # linear interp, rate 2/3\n",
    "                diff_feat = diff_feat[0:CFG.out_samples]\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, i] = diff_feat\n",
    "\n",
    "        n = CFG.n_map_features\n",
    "        if len(CFG.freq_channels) > 0:\n",
    "            for i in range(CFG.n_map_features):\n",
    "                diff_feat = X[:, i]\n",
    "                for j, (lowcut, highcut) in enumerate(CFG.freq_channels):\n",
    "                    band_feat = butter_bandpass_filter(\n",
    "                        diff_feat, lowcut, highcut, CFG.sampling_rate, order=CFG.filter_order,  # 6\n",
    "                    )\n",
    "                    X[:, n] = band_feat\n",
    "                    n += 1\n",
    "\n",
    "        for spml_feat in CFG.simple_features:\n",
    "            feat_val = data[:, CFG.feature_to_index[spml_feat]]\n",
    "\n",
    "            if not self.bandpass_filter is None:\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, n] = feat_val\n",
    "            n += 1\n",
    "\n",
    "        # Обрезать края превышающие значения [-1024, 1024]\n",
    "        X = np.clip(X, -1024, 1024)\n",
    "\n",
    "        # Замените NaN нулем и разделить все на 32\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "\n",
    "        # обрезать полосовым фильтром верхнюю границу в 20 Hz.\n",
    "        X = butter_lowpass_filter(X, order=CFG.filter_order)  # 4\n",
    "\n",
    "        y_prob = np.zeros(CFG.target_size, dtype=\"float32\")  # Size=(6,)\n",
    "        if self.mode != \"test\":\n",
    "            y_prob = row[CFG.target_cols].values.astype(np.float32)\n",
    "\n",
    "        return X, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivLossWithLogits(nn.KLDivLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        y = nn.functional.log_softmax(y, dim=1)\n",
    "        loss = super().forward(y, t)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True  # Это опция требует много паямяти GPU\n",
    "    # pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_1D_Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        downsampling,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n",
    "        # self.relu = nn.ReLU(inplace=False)\n",
    "        # self.relu_1 = nn.PReLU()\n",
    "        # self.relu_2 = nn.PReLU()\n",
    "        self.relu_1 = nn.Hardswish()\n",
    "        self.relu_2 = nn.Hardswish()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "        identity = self.downsampling(x)\n",
    "\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels,\n",
    "        in_channels,\n",
    "        fixed_kernel_size,\n",
    "        num_classes,\n",
    "        linear_layer_features,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "    ):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.planes = 24\n",
    "        self.parallel_conv = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        for i, kernel_size in enumerate(list(self.kernels)):\n",
    "            sep_conv = nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=self.planes,\n",
    "                kernel_size=(kernel_size),\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.parallel_conv.append(sep_conv)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        # self.relu = nn.ReLU(inplace=False)\n",
    "        # self.relu_1 = nn.ReLU()\n",
    "        # self.relu_2 = nn.ReLU()\n",
    "        self.relu_1 = nn.SiLU()\n",
    "        self.relu_2 = nn.SiLU()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=self.planes,\n",
    "            out_channels=self.planes,\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.block = self._make_resnet_layer(\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=1,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            padding=fixed_kernel_size // 2,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.in_channels,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            # dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(in_features=linear_layer_features, out_features=num_classes)\n",
    "\n",
    "    def _make_resnet_layer(\n",
    "        self,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        blocks=9,\n",
    "        padding=0,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        base_width = self.planes\n",
    "\n",
    "        for i in range(blocks):\n",
    "            downsampling = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "            )\n",
    "            layers.append(\n",
    "                ResNet_1D_Block(\n",
    "                    in_channels=self.planes,\n",
    "                    out_channels=self.planes,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    downsampling=downsampling,\n",
    "                    dilation=dilation,\n",
    "                    groups=groups,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out_sep = []\n",
    "        for i in range(len(self.kernels)):\n",
    "            sep = self.parallel_conv[i](x)\n",
    "            out_sep.append(sep)\n",
    "\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.block(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.avgpool(out)\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n",
    "        new_rnn_h = rnn_out[:, -1, :]\n",
    "\n",
    "        new_out = torch.cat([out, new_rnn_h], dim=1)\n",
    "        return new_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_out = self.extract_features(x)\n",
    "        result = self.fc(new_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adan Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adan(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for computing\n",
    "            running averages of gradient and its norm. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay (default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.98, 0.92, 0.99),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.2,\n",
    "        max_grad_norm=0.0,\n",
    "        no_prox=False,\n",
    "    ):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError(\"Invalid Max grad norm: {}\".format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            no_prox=no_prox,\n",
    "        )\n",
    "        super(Adan, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"no_prox\", False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group[\"step\"] = 0\n",
    "            for p in group[\"params\"]:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        if self.defaults[\"max_grad_norm\"] > 0:\n",
    "            device = self.param_groups[0][\"params\"][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults[\"max_grad_norm\"], device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group[\"params\"]:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group[\"eps\"]), max=1.0\n",
    "            )\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2, beta3 = group[\"betas\"]\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support by making it tensor, or pass list into kernel\n",
    "            if \"step\" in group:\n",
    "                group[\"step\"] += 1\n",
    "            else:\n",
    "                group[\"step\"] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1 ** group[\"step\"]\n",
    "            bias_correction2 = 1.0 - beta2 ** group[\"step\"]\n",
    "            bias_correction3 = 1.0 - beta3 ** group[\"step\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "                grad = p.grad.mul_(clip_global_grad_norm)\n",
    "                if \"pre_grad\" not in state or group[\"step\"] == 1:\n",
    "                    state[\"pre_grad\"] = grad\n",
    "\n",
    "                copy_grad = grad.clone()\n",
    "\n",
    "                exp_avg, exp_avg_sq, exp_avg_diff = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_avg_sq\"],\n",
    "                    state[\"exp_avg_diff\"],\n",
    "                )\n",
    "                diff = grad - state[\"pre_grad\"]\n",
    "\n",
    "                update = grad + beta2 * diff\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "                exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  # diff_t\n",
    "                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  # n_t\n",
    "\n",
    "                denom = ((exp_avg_sq).sqrt() / math.sqrt(bias_correction3)).add_(\n",
    "                    group[\"eps\"]\n",
    "                )\n",
    "                update = (\n",
    "                    (\n",
    "                        exp_avg / bias_correction1\n",
    "                        + beta2 * exp_avg_diff / bias_correction2\n",
    "                    )\n",
    "                ).div_(denom)\n",
    "\n",
    "                if group[\"no_prox\"]:\n",
    "                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                else:\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                    p.data.div_(1 + group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                state[\"pre_grad\"] = copy_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    stage, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(train_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(train_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} [{step}/{len(train_loader)}] Loss: {losses.val:.4f} Loss Avg:{losses.avg:.4f}\"\n",
    "            )\n",
    "            # \"Elapsed {remain:s} Grad: {grad_norm:.4f}  LR: {cheduler.get_lr()[0]:.8f}\"\n",
    "\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] loss\": losses.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0],\n",
    "                }\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(stage, epoch, valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(nn.Softmax(dim=1)(y_preds).to(\"cpu\").numpy())\n",
    "        targets.append(labels.to(\"cpu\").numpy())\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(valid_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(valid_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} VALIDATION: [{step}/{len(valid_loader)}] Val Loss: {losses.val:.4f} Val Loss Avg: {losses.avg:.4f}\"\n",
    "            )\n",
    "            # Elapsed {remain:s}\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(cfg, model, device, epochs, num_batches_per_epoch):\n",
    "    lr = cfg.lr\n",
    "    # lr = default_configs[\"lr\"]\n",
    "    if cfg.optimizer == \"SAM\":\n",
    "        base_optimizer = (\n",
    "            torch.optim.SGD\n",
    "        )  # define an optimizer for the \"sharpness-aware\" update\n",
    "        optimizer_model = SAM(\n",
    "            model.parameters(),\n",
    "            base_optimizer,\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            adaptive=True,\n",
    "        )\n",
    "    elif cfg.optimizer == \"Ranger21\":\n",
    "        optimizer_model = Ranger21(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            num_epochs=epochs,\n",
    "            num_batches_per_epoch=num_batches_per_epoch,\n",
    "        )\n",
    "    elif cfg.optimizer == \"SGD\":\n",
    "        optimizer_model = torch.optim.SGD(\n",
    "            model.parameters(), lr=lr, weight_decay=cfg.weight_decay, momentum=0.9\n",
    "        )\n",
    "    elif cfg.optimizer == \"Adam\":\n",
    "        optimizer_model = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n",
    "    elif cfg.optimizer == \"AdamW\":\n",
    "        optimizer_model = AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=CFG.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == \"Lion\":\n",
    "        optimizer_model = Lion(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == \"Adan\":\n",
    "        optimizer_model = Adan(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    return optimizer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, epochs, steps_per_epoch):\n",
    "    if CFG.scheduler == \"ReduceLROnPlateau\":\n",
    "        scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingWarmRestarts\":\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.cosanneal_res_params)\n",
    "    elif CFG.scheduler == \"OneCycleLR\":\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            pct_start=0.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            max_lr=CFG.lr,\n",
    "            div_factor=25,\n",
    "            final_div_factor=4.0e-01,\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(stage, epochs, folds, fold, directory, prev_dir, eggs):\n",
    "    train_folds = folds[folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "    train_dataset = EEGDataset(\n",
    "        train_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"train\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "\n",
    "    valid_dataset = EEGDataset(\n",
    "        valid_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"valid\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        #rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size * CFG.batch_koef_valid,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    LOGGER.info(\n",
    "        f\"========== stage: {stage} fold: {fold} training {len(train_loader)} / {len(valid_loader)} ==========\"\n",
    "    )\n",
    "\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "\n",
    "    if stage > 1:\n",
    "        model_weight = f\"{prev_dir}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage-1}_fold-{fold}_best.pth\"\n",
    "        checkpoint = torch.load(model_weight, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # CPMP: wrap the model to use all GPUs\n",
    "    if CFG.parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = build_optimizer(\n",
    "        CFG, model, device, epochs=epochs, num_batches_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scheduler = get_scheduler(\n",
    "        optimizer, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_score = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(\n",
    "            stage,\n",
    "            fold,\n",
    "            train_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # eval\n",
    "        valid_dataset.set_offset(CFG.sample_offset)\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            stage,\n",
    "            epoch,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        avg_loss_line = ''\n",
    "        if CFG.multi_validation:\n",
    "            multi_avg_val_loss = np.zeros(CFG.n_split_samples)\n",
    "            start = (2 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            finish = (3 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            delta = (finish - start) // 5\n",
    "            for i in range(CFG.n_split_samples):\n",
    "                valid_dataset.set_offset(start)\n",
    "                multi_avg_val_loss[i], _ = valid_fn(\n",
    "                    stage,\n",
    "                    epoch,\n",
    "                    valid_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                )\n",
    "                avg_loss_line += f\" {multi_avg_val_loss[i]:.4f}\"\n",
    "                start += delta\n",
    "            avg_loss_line += f\" mean={np.mean(multi_avg_val_loss):.4f}\"\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} Avg Train Loss: {avg_loss:.4f} Avg Valid Loss: {avg_val_loss:.4f} / {avg_loss_line}\"\n",
    "        )\n",
    "        #   time: {elapsed:.0f}s\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] stage\": stage,\n",
    "                    f\"[fold{fold}] epoch\": epoch + 1,\n",
    "                    f\"[fold{fold}] avg_train_loss\": avg_loss,\n",
    "                    f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                    #f\"[fold{fold}] score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if CFG.save_all_models:\n",
    "            torch.save(\n",
    "                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_epoch-{epoch}_val-{avg_val_loss:.4f}_train-{avg_loss:.4f}.pth\",\n",
    "            )\n",
    "\n",
    "        if best_score > avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            LOGGER.info(f\"Epoch {epoch+1} Save Best Valid Loss: {avg_val_loss:.4f}\")\n",
    "            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n",
    "            torch.save(\n",
    "                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "\n",
    "    # valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "    valid_folds[CFG.pred_cols] = predictions\n",
    "    valid_folds[CFG.target_cols] = valid_labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (106800, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "There are 1950 patients in the training data.\n",
      "There are 17089 EEG IDs in the training data.\n",
      "There are 20183 unique eeg_id + votes in the training data.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.file_train)\n",
    "TARGETS = train.columns[-6:]\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Targets\", list(TARGETS))\n",
    "\n",
    "train[\"total_evaluators\"] = train[CFG.target_cols].sum(axis=1)\n",
    "\n",
    "train_uniq = train.drop_duplicates(subset=[\"eeg_id\"] + list(TARGETS))\n",
    "\n",
    "print(f\"There are {train.patient_id.nunique()} patients in the training data.\")\n",
    "print(f\"There are {train.eeg_id.nunique()} EEG IDs in the training data.\")\n",
    "print(f\"There are {train_uniq.shape[0]} unique eeg_id + votes in the training data.\")\n",
    "\n",
    "if CFG.visualize:\n",
    "    train_uniq.eeg_id.value_counts().value_counts().plot(\n",
    "        kind=\"bar\",\n",
    "        title=f\"Distribution of Count of EEG w Unique Vote: \"\n",
    "        f\"{train_uniq.shape[0]} examples\",\n",
    "    )\n",
    "\n",
    "\n",
    "del train_uniq\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 raw eeg features\n",
      "['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n"
     ]
    }
   ],
   "source": [
    "if CFG.visualize:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "tst_eeg_df = pd.read_parquet(CFG.file_features_test)\n",
    "tst_eeg_features = tst_eeg_df.columns\n",
    "print(f\"There are {len(tst_eeg_features)} raw eeg features\")\n",
    "print(list(tst_eeg_features))\n",
    "del tst_eeg_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20183\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPD</th>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPD</th>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seizure</th>\n",
       "      <th>4</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   eeg_id  eeg_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GPD              4                     96          96   \n",
       "GRDA             4                     10          10   \n",
       "LPD              4                    250         250   \n",
       "LRDA             4                     32          32   \n",
       "Other            4                    172         172   \n",
       "Seizure          4                    192         192   \n",
       "\n",
       "                                   eeg_label_offset_seconds  spectrogram_id  \\\n",
       "expert_consensus total_evaluators                                             \n",
       "GPD              4                                       96              96   \n",
       "GRDA             4                                       10              10   \n",
       "LPD              4                                      250             250   \n",
       "LRDA             4                                       32              32   \n",
       "Other            4                                      172             172   \n",
       "Seizure          4                                      192             192   \n",
       "\n",
       "                                   spectrogram_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GPD              4                                 96   \n",
       "GRDA             4                                 10   \n",
       "LPD              4                                250   \n",
       "LRDA             4                                 32   \n",
       "Other            4                                172   \n",
       "Seizure          4                                192   \n",
       "\n",
       "                                   spectrogram_label_offset_seconds  label_id  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GPD              4                                               96        96   \n",
       "GRDA             4                                               10        10   \n",
       "LPD              4                                              250       250   \n",
       "LRDA             4                                               32        32   \n",
       "Other            4                                              172       172   \n",
       "Seizure          4                                              192       192   \n",
       "\n",
       "                                   patient_id  seizure_vote  lpd_vote  \\\n",
       "expert_consensus total_evaluators                                       \n",
       "GPD              4                         96            96        96   \n",
       "GRDA             4                         10            10        10   \n",
       "LPD              4                        250           250       250   \n",
       "LRDA             4                         32            32        32   \n",
       "Other            4                        172           172       172   \n",
       "Seizure          4                        192           192       192   \n",
       "\n",
       "                                   gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GPD              4                       96         96         96          96   \n",
       "GRDA             4                       10         10         10          10   \n",
       "LPD              4                      250        250        250         250   \n",
       "LRDA             4                       32         32         32          32   \n",
       "Other            4                      172        172        172         172   \n",
       "Seizure          4                      192        192        192         192   \n",
       "\n",
       "                                   target  \n",
       "expert_consensus total_evaluators          \n",
       "GPD              4                     96  \n",
       "GRDA             4                     10  \n",
       "LPD              4                    250  \n",
       "LRDA             4                     32  \n",
       "Other            4                    172  \n",
       "Seizure          4                    192  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "all_eeg_specs = np.load(CFG.file_eeg_specs, allow_pickle=True).item()\n",
    "\n",
    "train = train[train[\"label_id\"].isin(all_eeg_specs.keys())].copy()\n",
    "print(train.shape[0])\n",
    "\n",
    "y_data = train[TARGETS].values + 0.166666667  # Regularization value\n",
    "y_data = y_data / y_data.sum(axis=1, keepdims=True)\n",
    "train[TARGETS] = y_data\n",
    "\n",
    "train[\"target\"] = train[\"expert_consensus\"]\n",
    "\n",
    "train[train['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12546\n",
      "6492\n"
     ]
    }
   ],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    train['key_id'] = range(train.shape[0])\n",
    "\n",
    "    train_pop_olds = []\n",
    "    for total_eval in CFG.total_evals_old:\n",
    "        if type(total_eval) is list:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[0][1]\n",
    "            ) | (train[\"total_evaluators\"] >= total_eval[1][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1][1]\n",
    "            )\n",
    "        else:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1]\n",
    "            )\n",
    "\n",
    "        train_pop = train[pop_idx].copy().reset_index()\n",
    "\n",
    "        sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "        train_pop[\"fold\"] = -1\n",
    "        for fold_id, (_, val_idx) in enumerate(\n",
    "            sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "        ):\n",
    "            train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "        train_pop_olds.append(train_pop)\n",
    "        print(train_pop.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13595\n",
      "6492\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "      <th>key_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPD</th>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRDA</th>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seizure</th>\n",
       "      <th>4</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   index  eeg_id  eeg_sub_id  \\\n",
       "expert_consensus total_evaluators                              \n",
       "GRDA             4                    10      10          10   \n",
       "LPD              4                   250     250         250   \n",
       "LRDA             4                    32      32          32   \n",
       "Other            4                   172     172         172   \n",
       "Seizure          4                   192     192         192   \n",
       "\n",
       "                                   eeg_label_offset_seconds  spectrogram_id  \\\n",
       "expert_consensus total_evaluators                                             \n",
       "GRDA             4                                       10              10   \n",
       "LPD              4                                      250             250   \n",
       "LRDA             4                                       32              32   \n",
       "Other            4                                      172             172   \n",
       "Seizure          4                                      192             192   \n",
       "\n",
       "                                   spectrogram_sub_id  \\\n",
       "expert_consensus total_evaluators                       \n",
       "GRDA             4                                 10   \n",
       "LPD              4                                250   \n",
       "LRDA             4                                 32   \n",
       "Other            4                                172   \n",
       "Seizure          4                                192   \n",
       "\n",
       "                                   spectrogram_label_offset_seconds  label_id  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GRDA             4                                               10        10   \n",
       "LPD              4                                              250       250   \n",
       "LRDA             4                                               32        32   \n",
       "Other            4                                              172       172   \n",
       "Seizure          4                                              192       192   \n",
       "\n",
       "                                   patient_id  seizure_vote  lpd_vote  \\\n",
       "expert_consensus total_evaluators                                       \n",
       "GRDA             4                         10            10        10   \n",
       "LPD              4                        250           250       250   \n",
       "LRDA             4                         32            32        32   \n",
       "Other            4                        172           172       172   \n",
       "Seizure          4                        192           192       192   \n",
       "\n",
       "                                   gpd_vote  lrda_vote  grda_vote  other_vote  \\\n",
       "expert_consensus total_evaluators                                               \n",
       "GRDA             4                       10         10         10          10   \n",
       "LPD              4                      250        250        250         250   \n",
       "LRDA             4                       32         32         32          32   \n",
       "Other            4                      172        172        172         172   \n",
       "Seizure          4                      192        192        192         192   \n",
       "\n",
       "                                   target  key_id  fold  \n",
       "expert_consensus total_evaluators                        \n",
       "GRDA             4                     10      10    10  \n",
       "LPD              4                    250     250   250  \n",
       "LRDA             4                     32      32    32  \n",
       "Other            4                    172     172   172  \n",
       "Seizure          4                    192     192   192  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pops = []\n",
    "for eval_list in CFG.total_evaluators:\n",
    "    result=[]\n",
    "    train_pop = train\n",
    "    for eval_dict in eval_list:\n",
    "        band = eval_dict['band']\n",
    "        pop_idx = (train_pop[\"total_evaluators\"] >= band[0])\n",
    "        pop_idx &= (train_pop[\"total_evaluators\"] <= band[1])\n",
    "        for exclude in eval_dict['excl_evals']:\n",
    "            pop_idx &= ~(train_pop['expert_consensus'] == exclude)\n",
    "            pass\n",
    "        result.append(train_pop[pop_idx])\n",
    "    train_pop = pd.concat(result).copy().reset_index()\n",
    "\n",
    "    sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "    train_pop[\"fold\"] = -1\n",
    "    for fold_id, (_, val_idx) in enumerate(\n",
    "        sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "    ):\n",
    "        train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "    train_pops.append(train_pop)\n",
    "    print(train_pop.shape[0])\n",
    "\n",
    "train_0 = train_pops[0]\n",
    "train_0[train_0['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th>target</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>1626798710</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219001</td>\n",
       "      <td>2</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3631726128</td>\n",
       "      <td>23435</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>2529955608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219001</td>\n",
       "      <td>4</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4265493714</td>\n",
       "      <td>23435</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62</td>\n",
       "      <td>989810287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2843061</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116172961</td>\n",
       "      <td>13521</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>5</td>\n",
       "      <td>Other</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>172</td>\n",
       "      <td>4000022002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7122706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462363590</td>\n",
       "      <td>11471</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>177</td>\n",
       "      <td>3429523414</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7236473</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1420869020</td>\n",
       "      <td>24909</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13661</th>\n",
       "      <td>106554</td>\n",
       "      <td>2293242511</td>\n",
       "      <td>21</td>\n",
       "      <td>188.0</td>\n",
       "      <td>2141866254</td>\n",
       "      <td>21</td>\n",
       "      <td>188.0</td>\n",
       "      <td>173246725</td>\n",
       "      <td>53838</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13673</th>\n",
       "      <td>106632</td>\n",
       "      <td>4288875638</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3377992611</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13675</th>\n",
       "      <td>106653</td>\n",
       "      <td>728496959</td>\n",
       "      <td>17</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>21</td>\n",
       "      <td>434.0</td>\n",
       "      <td>2948298992</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13676</th>\n",
       "      <td>106662</td>\n",
       "      <td>728496959</td>\n",
       "      <td>26</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2145358771</td>\n",
       "      <td>30</td>\n",
       "      <td>586.0</td>\n",
       "      <td>1610933349</td>\n",
       "      <td>200</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13686</th>\n",
       "      <td>106770</td>\n",
       "      <td>3349371726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2146170054</td>\n",
       "      <td>7</td>\n",
       "      <td>124.0</td>\n",
       "      <td>142522487</td>\n",
       "      <td>33380</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>5</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1241 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      eeg_id  eeg_sub_id  eeg_label_offset_seconds  \\\n",
       "3          30  1626798710           0                       0.0   \n",
       "4          32  2529955608           0                       0.0   \n",
       "11         62   989810287           0                       0.0   \n",
       "31        172  4000022002           0                       0.0   \n",
       "33        177  3429523414           3                      10.0   \n",
       "...       ...         ...         ...                       ...   \n",
       "13661  106554  2293242511          21                     188.0   \n",
       "13673  106632  4288875638           0                       0.0   \n",
       "13675  106653   728496959          17                     154.0   \n",
       "13676  106662   728496959          26                     306.0   \n",
       "13686  106770  3349371726           0                       0.0   \n",
       "\n",
       "       spectrogram_id  spectrogram_sub_id  spectrogram_label_offset_seconds  \\\n",
       "3             1219001                   2                              74.0   \n",
       "4             1219001                   4                             190.0   \n",
       "11            2843061                   0                               0.0   \n",
       "31            7122706                   0                               0.0   \n",
       "33            7236473                   3                              10.0   \n",
       "...               ...                 ...                               ...   \n",
       "13661      2141866254                  21                             188.0   \n",
       "13673      2145358771                   0                               0.0   \n",
       "13675      2145358771                  21                             434.0   \n",
       "13676      2145358771                  30                             586.0   \n",
       "13686      2146170054                   7                             124.0   \n",
       "\n",
       "         label_id  patient_id expert_consensus  seizure_vote  lpd_vote  \\\n",
       "3      3631726128       23435          Seizure      0.527778  0.027778   \n",
       "4      4265493714       23435          Seizure      0.527778  0.027778   \n",
       "11      116172961       13521            Other      0.027778  0.194444   \n",
       "31      462363590       11471          Seizure      0.527778  0.027778   \n",
       "33     1420869020       24909          Seizure      0.694444  0.027778   \n",
       "...           ...         ...              ...           ...       ...   \n",
       "13661   173246725       53838          Seizure      0.527778  0.194444   \n",
       "13673  3377992611         200          Seizure      0.694444  0.194444   \n",
       "13675  2948298992         200          Seizure      0.694444  0.194444   \n",
       "13676  1610933349         200          Seizure      0.861111  0.027778   \n",
       "13686   142522487       33380          Seizure      0.527778  0.027778   \n",
       "\n",
       "       gpd_vote  lrda_vote  grda_vote  other_vote  total_evaluators   target  \\\n",
       "3      0.361111   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "4      0.361111   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "11     0.027778   0.027778   0.027778    0.694444                 5    Other   \n",
       "31     0.027778   0.027778   0.027778    0.361111                 5  Seizure   \n",
       "33     0.027778   0.027778   0.027778    0.194444                 5  Seizure   \n",
       "...         ...        ...        ...         ...               ...      ...   \n",
       "13661  0.194444   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13673  0.027778   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13675  0.027778   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13676  0.027778   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "13686  0.361111   0.027778   0.027778    0.027778                 5  Seizure   \n",
       "\n",
       "            Exist  \n",
       "3      right_only  \n",
       "4      right_only  \n",
       "11     right_only  \n",
       "31     right_only  \n",
       "33     right_only  \n",
       "...           ...  \n",
       "13661  right_only  \n",
       "13673  right_only  \n",
       "13675  right_only  \n",
       "13676  right_only  \n",
       "13686  right_only  \n",
       "\n",
       "[1241 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    df_old = train_pop_olds[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "    df_new = train_pops[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "\n",
    "    #outer merge the two DataFrames, adding an indicator column called 'Exist'\n",
    "    diff_df = pd.merge(df_old, df_new, how='outer', indicator='Exist')\n",
    "\n",
    "    #find which rows don't exist in both DataFrames\n",
    "    diff_df = diff_df.loc[diff_df['Exist'] != 'both']\n",
    "    display(diff_df)\n",
    "\n",
    "    del df_old, df_new, diff_df, train_pop_olds\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    print(\"Pop 1: train unique eeg_id + votes shape:\", train_pops[0].shape)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "del all_eeg_specs\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate train eeg id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "if CFG.create_eegs:\n",
    "    all_eegs = {}\n",
    "    visualize = 1 if CFG.visualize else 0\n",
    "    eeg_ids = train.eeg_id.unique()\n",
    "\n",
    "    for i, eeg_id in tqdm(enumerate(eeg_ids)):\n",
    "\n",
    "        # Сохранить ЭЭГ в словаре Python для массивов numpy\n",
    "        eeg_path = CFG.path_train / f\"{eeg_id}.parquet\"\n",
    "\n",
    "        # Вырезаем среднюю 50 секундную часть и заполняем по среднему Nan\n",
    "        data = eeg_from_parquet(eeg_path, display=i < visualize)\n",
    "        all_eegs[eeg_id] = data\n",
    "\n",
    "        if i == visualize:\n",
    "            if CFG.create_eegs:\n",
    "                print(\n",
    "                    f\"Processing {train['eeg_id'].nunique()} eeg parquets... \", end=\"\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Reading {len(eeg_ids)} eeg NumPys from disk.\")\n",
    "                break\n",
    "    np.save(\"./eegs\", all_eegs)\n",
    "\n",
    "else:\n",
    "    all_eegs = np.load(CFG.file_raw_eeg, allow_pickle=True).item()\n",
    "\n",
    "if CFG.visualize:\n",
    "    frequencies = [1, 2, 4, 8, 16][::-1]  # frequencies in Hz\n",
    "    x = [all_eegs[eeg_ids[0]][:, 0]]  # select one EEG feature\n",
    "\n",
    "    for frequency in frequencies:\n",
    "        x.append(butter_lowpass_filter(x[0], cutoff_freq=frequency))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(CFG.nsamples), x[0], label=\"without filter\")\n",
    "    for k in range(1, len(x)):\n",
    "        plt.plot(\n",
    "            range(CFG.nsamples),\n",
    "            x[k] - k * (x[0].max() - x[0].min()),\n",
    "            label=f\"with filter {frequencies[k-1]}Hz\",\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Butter Low-Pass Filter Examples\", size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    train_dataset = EEGDataset(\n",
    "        train_pops[0], batch_size=CFG.batch_size, eegs=all_eegs, mode=\"train\"\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    output = train_dataset[0]\n",
    "    X, y = output[\"eeg\"], output[\"labels\"]\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    iot = torch.randn(2, CFG.nsamples, CFG.in_channels)  # .cuda()\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "    output = model(iot)\n",
    "    print(output.shape)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X = batch.pop(\"eeg\")\n",
    "        y = batch.pop(\"labels\")\n",
    "        for item in range(4):\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            offset = 0\n",
    "            for col in range(X.shape[-1]):\n",
    "                if col != 0:\n",
    "                    offset -= X[item, :, col].min()\n",
    "                plt.plot(\n",
    "                    range(CFG.nsamples),\n",
    "                    X[item, :, col] + offset,\n",
    "                    label=f\"feature {col+1}\",\n",
    "                )\n",
    "                offset += X[item, :, col].max()\n",
    "            tt = f\"{y[col][0]:0.1f}\"\n",
    "            for t in y[col][1:]:\n",
    "                tt += f\", {t:0.1f}\"\n",
    "            plt.title(f\"EEG_Id = {eeg_ids[item]}\\nTarget = {tt}\", size=14)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        break\n",
    "\n",
    "    del iot, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(preds, targets):\n",
    "    oof = pd.DataFrame(preds.copy())\n",
    "    oof[\"id\"] = np.arange(len(oof))\n",
    "    true = pd.DataFrame(targets.copy())\n",
    "    true[\"id\"] = np.arange(len(true))\n",
    "    cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "    return cv\n",
    "\n",
    "\n",
    "def get_result(result_df):\n",
    "    gt = result_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "    gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    gt.reset_index(inplace=True, drop=True)\n",
    "    preds = result_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "    preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "    preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    preds.reset_index(inplace=True, drop=True)\n",
    "    score_loss = get_score(gt[CFG.target_cols], preds[CFG.target_cols])\n",
    "    LOGGER.info(f\"Score with best loss weights: {score_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_stages:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    prev_dir = \"\"\n",
    "    for stage in range(len(CFG.total_evaluators)):\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        if not os.path.exists(pop_dir):\n",
    "            os.makedirs(pop_dir)\n",
    "\n",
    "        if stage not in CFG.train_stages:\n",
    "            prev_dir = pop_dir\n",
    "            continue\n",
    "\n",
    "        oof_df = pd.DataFrame()\n",
    "        scores = []\n",
    "        for fold in CFG.train_folds:\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            oof_df = pd.concat([oof_df, train_oof_df])\n",
    "            scores.append(score)\n",
    "\n",
    "            LOGGER.info(f\"========== stage: {stage+1} fold: {fold} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "        LOGGER.info(f\"==================== CV ====================\")\n",
    "        LOGGER.info(f\"Score with best loss weights: {np.mean(scores):.4f}\")\n",
    "\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        prev_dir = pop_dir\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== stage: 1 fold: 0 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5495 Avg Valid Loss: 0.6508 / \n",
      "Epoch 1 Save Best Valid Loss: 0.6508\n",
      "Epoch 2 Avg Train Loss: 0.4590 Avg Valid Loss: 0.4924 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4924\n",
      "Epoch 3 Avg Train Loss: 0.4293 Avg Valid Loss: 0.4963 / \n",
      "Epoch 4 Avg Train Loss: 0.4150 Avg Valid Loss: 0.4782 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4782\n",
      "Epoch 5 Avg Train Loss: 0.4075 Avg Valid Loss: 0.4760 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4760\n",
      "Epoch 6 Avg Train Loss: 0.4005 Avg Valid Loss: 0.4862 / \n",
      "Epoch 7 Avg Train Loss: 0.3914 Avg Valid Loss: 0.4634 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4634\n",
      "Epoch 8 Avg Train Loss: 0.3936 Avg Valid Loss: 0.4597 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4597\n",
      "Epoch 9 Avg Train Loss: 0.3887 Avg Valid Loss: 0.4771 / \n",
      "Epoch 10 Avg Train Loss: 0.3839 Avg Valid Loss: 0.4817 / \n",
      "Epoch 11 Avg Train Loss: 0.3817 Avg Valid Loss: 0.4710 / \n",
      "Epoch 12 Avg Train Loss: 0.3759 Avg Valid Loss: 0.4536 / \n",
      "Epoch 12 Save Best Valid Loss: 0.4536\n",
      "Epoch 13 Avg Train Loss: 0.3787 Avg Valid Loss: 0.4653 / \n",
      "Epoch 14 Avg Train Loss: 0.3735 Avg Valid Loss: 0.4718 / \n",
      "Epoch 15 Avg Train Loss: 0.3759 Avg Valid Loss: 0.4440 / \n",
      "Epoch 15 Save Best Valid Loss: 0.4440\n",
      "Epoch 16 Avg Train Loss: 0.3702 Avg Valid Loss: 0.4622 / \n",
      "Epoch 17 Avg Train Loss: 0.3618 Avg Valid Loss: 0.4655 / \n",
      "Epoch 18 Avg Train Loss: 0.3667 Avg Valid Loss: 0.4405 / \n",
      "Epoch 18 Save Best Valid Loss: 0.4405\n",
      "Epoch 19 Avg Train Loss: 0.3639 Avg Valid Loss: 0.4886 / \n",
      "Epoch 20 Avg Train Loss: 0.3636 Avg Valid Loss: 0.4630 / \n",
      "Epoch 21 Avg Train Loss: 0.3656 Avg Valid Loss: 0.4598 / \n",
      "Epoch 22 Avg Train Loss: 0.3554 Avg Valid Loss: 0.4519 / \n",
      "Epoch 23 Avg Train Loss: 0.3579 Avg Valid Loss: 0.4672 / \n",
      "Epoch 24 Avg Train Loss: 0.3588 Avg Valid Loss: 0.4397 / \n",
      "Epoch 24 Save Best Valid Loss: 0.4397\n",
      "Epoch 25 Avg Train Loss: 0.3570 Avg Valid Loss: 0.4588 / \n",
      "Epoch 26 Avg Train Loss: 0.3560 Avg Valid Loss: 0.4350 / \n",
      "Epoch 26 Save Best Valid Loss: 0.4350\n",
      "Epoch 27 Avg Train Loss: 0.3495 Avg Valid Loss: 0.4511 / \n",
      "Epoch 28 Avg Train Loss: 0.3472 Avg Valid Loss: 0.4525 / \n",
      "Epoch 29 Avg Train Loss: 0.3496 Avg Valid Loss: 0.4599 / \n",
      "Epoch 30 Avg Train Loss: 0.3500 Avg Valid Loss: 0.4521 / \n",
      "Epoch 31 Avg Train Loss: 0.3491 Avg Valid Loss: 0.4509 / \n",
      "Epoch 32 Avg Train Loss: 0.3512 Avg Valid Loss: 0.4453 / \n",
      "Epoch 33 Avg Train Loss: 0.3457 Avg Valid Loss: 0.4468 / \n",
      "Epoch 34 Avg Train Loss: 0.3485 Avg Valid Loss: 0.4407 / \n",
      "Epoch 35 Avg Train Loss: 0.3540 Avg Valid Loss: 0.4437 / \n",
      "Epoch 36 Avg Train Loss: 0.3490 Avg Valid Loss: 0.4576 / \n",
      "Epoch 37 Avg Train Loss: 0.3466 Avg Valid Loss: 0.4554 / \n",
      "Epoch 38 Avg Train Loss: 0.3461 Avg Valid Loss: 0.4394 / \n",
      "Epoch 39 Avg Train Loss: 0.3421 Avg Valid Loss: 0.4624 / \n",
      "Epoch 40 Avg Train Loss: 0.3404 Avg Valid Loss: 0.4278 / \n",
      "Epoch 40 Save Best Valid Loss: 0.4278\n",
      "Epoch 41 Avg Train Loss: 0.3431 Avg Valid Loss: 0.4773 / \n",
      "Epoch 42 Avg Train Loss: 0.3442 Avg Valid Loss: 0.4390 / \n",
      "Epoch 43 Avg Train Loss: 0.3392 Avg Valid Loss: 0.4596 / \n",
      "Epoch 44 Avg Train Loss: 0.3357 Avg Valid Loss: 0.4683 / \n",
      "Epoch 45 Avg Train Loss: 0.3390 Avg Valid Loss: 0.4365 / \n",
      "Epoch 46 Avg Train Loss: 0.3436 Avg Valid Loss: 0.4506 / \n",
      "Epoch 47 Avg Train Loss: 0.3366 Avg Valid Loss: 0.4488 / \n",
      "Epoch 48 Avg Train Loss: 0.3369 Avg Valid Loss: 0.4710 / \n",
      "Epoch 49 Avg Train Loss: 0.3371 Avg Valid Loss: 0.4455 / \n",
      "Epoch 50 Avg Train Loss: 0.3342 Avg Valid Loss: 0.4504 / \n",
      "========== fold: 0 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.4278\n",
      "========== stage: 2 fold: 0 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3925 Avg Valid Loss: 0.3929 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3929\n",
      "Epoch 2 Avg Train Loss: 0.3627 Avg Valid Loss: 0.3753 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3753\n",
      "Epoch 3 Avg Train Loss: 0.3552 Avg Valid Loss: 0.3568 / \n",
      "Epoch 3 Save Best Valid Loss: 0.3568\n",
      "Epoch 4 Avg Train Loss: 0.3447 Avg Valid Loss: 0.3447 / \n",
      "Epoch 4 Save Best Valid Loss: 0.3447\n",
      "Epoch 5 Avg Train Loss: 0.3462 Avg Valid Loss: 0.3841 / \n",
      "Epoch 6 Avg Train Loss: 0.3364 Avg Valid Loss: 0.3491 / \n",
      "Epoch 7 Avg Train Loss: 0.3404 Avg Valid Loss: 0.3514 / \n",
      "Epoch 8 Avg Train Loss: 0.3372 Avg Valid Loss: 0.3706 / \n",
      "Epoch 9 Avg Train Loss: 0.3356 Avg Valid Loss: 0.3691 / \n",
      "Epoch 10 Avg Train Loss: 0.3314 Avg Valid Loss: 0.3467 / \n",
      "Epoch 11 Avg Train Loss: 0.3271 Avg Valid Loss: 0.3600 / \n",
      "Epoch 12 Avg Train Loss: 0.3223 Avg Valid Loss: 0.3369 / \n",
      "Epoch 12 Save Best Valid Loss: 0.3369\n",
      "Epoch 13 Avg Train Loss: 0.3226 Avg Valid Loss: 0.3231 / \n",
      "Epoch 13 Save Best Valid Loss: 0.3231\n",
      "Epoch 14 Avg Train Loss: 0.3227 Avg Valid Loss: 0.3585 / \n",
      "Epoch 15 Avg Train Loss: 0.3229 Avg Valid Loss: 0.3456 / \n",
      "Epoch 16 Avg Train Loss: 0.3186 Avg Valid Loss: 0.3308 / \n",
      "Epoch 17 Avg Train Loss: 0.3213 Avg Valid Loss: 0.3696 / \n",
      "Epoch 18 Avg Train Loss: 0.3167 Avg Valid Loss: 0.3398 / \n",
      "Epoch 19 Avg Train Loss: 0.3178 Avg Valid Loss: 0.3396 / \n",
      "Epoch 20 Avg Train Loss: 0.3117 Avg Valid Loss: 0.3423 / \n",
      "Epoch 21 Avg Train Loss: 0.3066 Avg Valid Loss: 0.3333 / \n",
      "Epoch 22 Avg Train Loss: 0.3117 Avg Valid Loss: 0.3318 / \n",
      "Epoch 23 Avg Train Loss: 0.3126 Avg Valid Loss: 0.3354 / \n",
      "Epoch 24 Avg Train Loss: 0.3087 Avg Valid Loss: 0.3390 / \n",
      "Epoch 25 Avg Train Loss: 0.3129 Avg Valid Loss: 0.3368 / \n",
      "Epoch 26 Avg Train Loss: 0.3106 Avg Valid Loss: 0.3427 / \n",
      "Epoch 27 Avg Train Loss: 0.3080 Avg Valid Loss: 0.3293 / \n",
      "Epoch 28 Avg Train Loss: 0.3067 Avg Valid Loss: 0.3470 / \n",
      "Epoch 29 Avg Train Loss: 0.3122 Avg Valid Loss: 0.3344 / \n",
      "Epoch 30 Avg Train Loss: 0.3089 Avg Valid Loss: 0.3196 / \n",
      "Epoch 30 Save Best Valid Loss: 0.3196\n",
      "Epoch 31 Avg Train Loss: 0.3089 Avg Valid Loss: 0.3556 / \n",
      "Epoch 32 Avg Train Loss: 0.3106 Avg Valid Loss: 0.3441 / \n",
      "Epoch 33 Avg Train Loss: 0.3049 Avg Valid Loss: 0.3273 / \n",
      "Epoch 34 Avg Train Loss: 0.3031 Avg Valid Loss: 0.3329 / \n",
      "Epoch 35 Avg Train Loss: 0.3106 Avg Valid Loss: 0.3104 / \n",
      "Epoch 35 Save Best Valid Loss: 0.3104\n",
      "Epoch 36 Avg Train Loss: 0.3044 Avg Valid Loss: 0.3371 / \n",
      "Epoch 37 Avg Train Loss: 0.3033 Avg Valid Loss: 0.3331 / \n",
      "Epoch 38 Avg Train Loss: 0.2998 Avg Valid Loss: 0.3042 / \n",
      "Epoch 38 Save Best Valid Loss: 0.3042\n",
      "Epoch 39 Avg Train Loss: 0.2981 Avg Valid Loss: 0.3406 / \n",
      "Epoch 40 Avg Train Loss: 0.3018 Avg Valid Loss: 0.3298 / \n",
      "Epoch 41 Avg Train Loss: 0.2952 Avg Valid Loss: 0.3175 / \n",
      "Epoch 42 Avg Train Loss: 0.3010 Avg Valid Loss: 0.3333 / \n",
      "Epoch 43 Avg Train Loss: 0.3023 Avg Valid Loss: 0.3437 / \n",
      "Epoch 44 Avg Train Loss: 0.2980 Avg Valid Loss: 0.3589 / \n",
      "Epoch 45 Avg Train Loss: 0.3062 Avg Valid Loss: 0.3280 / \n",
      "Epoch 46 Avg Train Loss: 0.3004 Avg Valid Loss: 0.3318 / \n",
      "Epoch 47 Avg Train Loss: 0.2930 Avg Valid Loss: 0.3634 / \n",
      "Epoch 48 Avg Train Loss: 0.2972 Avg Valid Loss: 0.3388 / \n",
      "Epoch 49 Avg Train Loss: 0.2898 Avg Valid Loss: 0.3198 / \n",
      "Epoch 50 Avg Train Loss: 0.2978 Avg Valid Loss: 0.3371 / \n",
      "Epoch 51 Avg Train Loss: 0.3001 Avg Valid Loss: 0.3368 / \n",
      "Epoch 52 Avg Train Loss: 0.2938 Avg Valid Loss: 0.3566 / \n",
      "Epoch 53 Avg Train Loss: 0.2932 Avg Valid Loss: 0.3621 / \n",
      "Epoch 54 Avg Train Loss: 0.2959 Avg Valid Loss: 0.3682 / \n",
      "Epoch 55 Avg Train Loss: 0.3020 Avg Valid Loss: 0.3584 / \n",
      "Epoch 56 Avg Train Loss: 0.2965 Avg Valid Loss: 0.3296 / \n",
      "Epoch 57 Avg Train Loss: 0.2901 Avg Valid Loss: 0.3376 / \n",
      "Epoch 58 Avg Train Loss: 0.2900 Avg Valid Loss: 0.3178 / \n",
      "Epoch 59 Avg Train Loss: 0.2981 Avg Valid Loss: 0.3115 / \n",
      "Epoch 60 Avg Train Loss: 0.2887 Avg Valid Loss: 0.3053 / \n",
      "Epoch 61 Avg Train Loss: 0.2891 Avg Valid Loss: 0.3240 / \n",
      "Epoch 62 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3373 / \n",
      "Epoch 63 Avg Train Loss: 0.2890 Avg Valid Loss: 0.3224 / \n",
      "Epoch 64 Avg Train Loss: 0.2849 Avg Valid Loss: 0.3322 / \n",
      "Epoch 65 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3106 / \n",
      "Epoch 66 Avg Train Loss: 0.2832 Avg Valid Loss: 0.3340 / \n",
      "Epoch 67 Avg Train Loss: 0.2921 Avg Valid Loss: 0.3271 / \n",
      "Epoch 68 Avg Train Loss: 0.2971 Avg Valid Loss: 0.3198 / \n",
      "Epoch 69 Avg Train Loss: 0.2863 Avg Valid Loss: 0.3240 / \n",
      "Epoch 70 Avg Train Loss: 0.2865 Avg Valid Loss: 0.3220 / \n",
      "Epoch 71 Avg Train Loss: 0.2887 Avg Valid Loss: 0.3219 / \n",
      "Epoch 72 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3185 / \n",
      "Epoch 73 Avg Train Loss: 0.2842 Avg Valid Loss: 0.3214 / \n",
      "Epoch 74 Avg Train Loss: 0.2885 Avg Valid Loss: 0.3198 / \n",
      "Epoch 75 Avg Train Loss: 0.2866 Avg Valid Loss: 0.3180 / \n",
      "Epoch 76 Avg Train Loss: 0.2905 Avg Valid Loss: 0.3176 / \n",
      "Epoch 77 Avg Train Loss: 0.2910 Avg Valid Loss: 0.3189 / \n",
      "Epoch 78 Avg Train Loss: 0.2894 Avg Valid Loss: 0.3237 / \n",
      "Epoch 79 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3253 / \n",
      "Epoch 80 Avg Train Loss: 0.2855 Avg Valid Loss: 0.3203 / \n",
      "Epoch 81 Avg Train Loss: 0.2877 Avg Valid Loss: 0.3183 / \n",
      "Epoch 82 Avg Train Loss: 0.2850 Avg Valid Loss: 0.3220 / \n",
      "Epoch 83 Avg Train Loss: 0.2892 Avg Valid Loss: 0.3214 / \n",
      "Epoch 84 Avg Train Loss: 0.2871 Avg Valid Loss: 0.3286 / \n",
      "Epoch 85 Avg Train Loss: 0.2909 Avg Valid Loss: 0.3207 / \n",
      "Epoch 86 Avg Train Loss: 0.2899 Avg Valid Loss: 0.3191 / \n",
      "Epoch 87 Avg Train Loss: 0.2890 Avg Valid Loss: 0.3254 / \n",
      "Epoch 88 Avg Train Loss: 0.2907 Avg Valid Loss: 0.3199 / \n",
      "Epoch 89 Avg Train Loss: 0.2871 Avg Valid Loss: 0.3248 / \n",
      "Epoch 90 Avg Train Loss: 0.2805 Avg Valid Loss: 0.3219 / \n",
      "Epoch 91 Avg Train Loss: 0.2895 Avg Valid Loss: 0.3189 / \n",
      "Epoch 92 Avg Train Loss: 0.2894 Avg Valid Loss: 0.3184 / \n",
      "Epoch 93 Avg Train Loss: 0.2830 Avg Valid Loss: 0.3290 / \n",
      "Epoch 94 Avg Train Loss: 0.2902 Avg Valid Loss: 0.3190 / \n",
      "Epoch 95 Avg Train Loss: 0.2893 Avg Valid Loss: 0.3201 / \n",
      "Epoch 96 Avg Train Loss: 0.2931 Avg Valid Loss: 0.3191 / \n",
      "Epoch 97 Avg Train Loss: 0.2927 Avg Valid Loss: 0.3206 / \n",
      "Epoch 98 Avg Train Loss: 0.2864 Avg Valid Loss: 0.3207 / \n",
      "Epoch 99 Avg Train Loss: 0.2898 Avg Valid Loss: 0.3221 / \n",
      "Epoch 100 Avg Train Loss: 0.2823 Avg Valid Loss: 0.3229 / \n",
      "========== fold: 0 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3042\n",
      "========== stage: 1 fold: 1 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5620 Avg Valid Loss: 0.5281 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5281\n",
      "Epoch 2 Avg Train Loss: 0.4772 Avg Valid Loss: 0.4152 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4152\n",
      "Epoch 3 Avg Train Loss: 0.4500 Avg Valid Loss: 0.4479 / \n",
      "Epoch 4 Avg Train Loss: 0.4346 Avg Valid Loss: 0.4149 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4149\n",
      "Epoch 5 Avg Train Loss: 0.4276 Avg Valid Loss: 0.4083 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4083\n",
      "Epoch 6 Avg Train Loss: 0.4176 Avg Valid Loss: 0.3859 / \n",
      "Epoch 6 Save Best Valid Loss: 0.3859\n",
      "Epoch 7 Avg Train Loss: 0.4055 Avg Valid Loss: 0.3784 / \n",
      "Epoch 7 Save Best Valid Loss: 0.3784\n",
      "Epoch 8 Avg Train Loss: 0.4023 Avg Valid Loss: 0.3689 / \n",
      "Epoch 8 Save Best Valid Loss: 0.3689\n",
      "Epoch 9 Avg Train Loss: 0.4000 Avg Valid Loss: 0.3675 / \n",
      "Epoch 9 Save Best Valid Loss: 0.3675\n",
      "Epoch 10 Avg Train Loss: 0.3964 Avg Valid Loss: 0.3775 / \n",
      "Epoch 11 Avg Train Loss: 0.3931 Avg Valid Loss: 0.3614 / \n",
      "Epoch 11 Save Best Valid Loss: 0.3614\n",
      "Epoch 12 Avg Train Loss: 0.3926 Avg Valid Loss: 0.3601 / \n",
      "Epoch 12 Save Best Valid Loss: 0.3601\n",
      "Epoch 13 Avg Train Loss: 0.3904 Avg Valid Loss: 0.3588 / \n",
      "Epoch 13 Save Best Valid Loss: 0.3588\n",
      "Epoch 14 Avg Train Loss: 0.3825 Avg Valid Loss: 0.3685 / \n",
      "Epoch 15 Avg Train Loss: 0.3827 Avg Valid Loss: 0.3700 / \n",
      "Epoch 16 Avg Train Loss: 0.3774 Avg Valid Loss: 0.3687 / \n",
      "Epoch 17 Avg Train Loss: 0.3778 Avg Valid Loss: 0.3574 / \n",
      "Epoch 17 Save Best Valid Loss: 0.3574\n",
      "Epoch 18 Avg Train Loss: 0.3795 Avg Valid Loss: 0.3525 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3525\n",
      "Epoch 19 Avg Train Loss: 0.3789 Avg Valid Loss: 0.3601 / \n",
      "Epoch 20 Avg Train Loss: 0.3699 Avg Valid Loss: 0.3602 / \n",
      "Epoch 21 Avg Train Loss: 0.3740 Avg Valid Loss: 0.3655 / \n",
      "Epoch 22 Avg Train Loss: 0.3707 Avg Valid Loss: 0.3584 / \n",
      "Epoch 23 Avg Train Loss: 0.3667 Avg Valid Loss: 0.3726 / \n",
      "Epoch 24 Avg Train Loss: 0.3663 Avg Valid Loss: 0.3934 / \n",
      "Epoch 25 Avg Train Loss: 0.3672 Avg Valid Loss: 0.3421 / \n",
      "Epoch 25 Save Best Valid Loss: 0.3421\n",
      "Epoch 26 Avg Train Loss: 0.3644 Avg Valid Loss: 0.3499 / \n",
      "Epoch 27 Avg Train Loss: 0.3638 Avg Valid Loss: 0.3626 / \n",
      "Epoch 28 Avg Train Loss: 0.3670 Avg Valid Loss: 0.3753 / \n",
      "Epoch 29 Avg Train Loss: 0.3600 Avg Valid Loss: 0.3631 / \n",
      "Epoch 30 Avg Train Loss: 0.3596 Avg Valid Loss: 0.3853 / \n",
      "Epoch 31 Avg Train Loss: 0.3570 Avg Valid Loss: 0.3547 / \n",
      "Epoch 32 Avg Train Loss: 0.3608 Avg Valid Loss: 0.3579 / \n",
      "Epoch 33 Avg Train Loss: 0.3574 Avg Valid Loss: 0.3626 / \n",
      "Epoch 34 Avg Train Loss: 0.3564 Avg Valid Loss: 0.3481 / \n",
      "Epoch 35 Avg Train Loss: 0.3555 Avg Valid Loss: 0.3900 / \n",
      "Epoch 36 Avg Train Loss: 0.3536 Avg Valid Loss: 0.3540 / \n",
      "Epoch 37 Avg Train Loss: 0.3558 Avg Valid Loss: 0.3543 / \n",
      "Epoch 38 Avg Train Loss: 0.3528 Avg Valid Loss: 0.3637 / \n",
      "Epoch 39 Avg Train Loss: 0.3537 Avg Valid Loss: 0.3671 / \n",
      "Epoch 40 Avg Train Loss: 0.3498 Avg Valid Loss: 0.3765 / \n",
      "Epoch 41 Avg Train Loss: 0.3539 Avg Valid Loss: 0.3587 / \n",
      "Epoch 42 Avg Train Loss: 0.3498 Avg Valid Loss: 0.3704 / \n",
      "Epoch 43 Avg Train Loss: 0.3421 Avg Valid Loss: 0.3450 / \n",
      "Epoch 44 Avg Train Loss: 0.3482 Avg Valid Loss: 0.3455 / \n",
      "Epoch 45 Avg Train Loss: 0.3481 Avg Valid Loss: 0.3598 / \n",
      "Epoch 46 Avg Train Loss: 0.3438 Avg Valid Loss: 0.3458 / \n",
      "Epoch 47 Avg Train Loss: 0.3435 Avg Valid Loss: 0.3389 / \n",
      "Epoch 47 Save Best Valid Loss: 0.3389\n",
      "Epoch 48 Avg Train Loss: 0.3438 Avg Valid Loss: 0.3809 / \n",
      "Epoch 49 Avg Train Loss: 0.3469 Avg Valid Loss: 0.3515 / \n",
      "Epoch 50 Avg Train Loss: 0.3421 Avg Valid Loss: 0.3665 / \n",
      "========== fold: 1 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3389\n",
      "========== stage: 2 fold: 1 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3859 Avg Valid Loss: 0.3185 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3185\n",
      "Epoch 2 Avg Train Loss: 0.3570 Avg Valid Loss: 0.3147 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3147\n",
      "Epoch 3 Avg Train Loss: 0.3415 Avg Valid Loss: 0.3263 / \n",
      "Epoch 4 Avg Train Loss: 0.3356 Avg Valid Loss: 0.3263 / \n",
      "Epoch 5 Avg Train Loss: 0.3364 Avg Valid Loss: 0.3308 / \n",
      "Epoch 6 Avg Train Loss: 0.3286 Avg Valid Loss: 0.3196 / \n",
      "Epoch 7 Avg Train Loss: 0.3236 Avg Valid Loss: 0.3424 / \n",
      "Epoch 8 Avg Train Loss: 0.3242 Avg Valid Loss: 0.3221 / \n",
      "Epoch 9 Avg Train Loss: 0.3179 Avg Valid Loss: 0.3235 / \n",
      "Epoch 10 Avg Train Loss: 0.3193 Avg Valid Loss: 0.3229 / \n",
      "Epoch 11 Avg Train Loss: 0.3190 Avg Valid Loss: 0.3418 / \n",
      "Epoch 12 Avg Train Loss: 0.3196 Avg Valid Loss: 0.3420 / \n",
      "Epoch 13 Avg Train Loss: 0.3151 Avg Valid Loss: 0.3113 / \n",
      "Epoch 13 Save Best Valid Loss: 0.3113\n",
      "Epoch 14 Avg Train Loss: 0.3159 Avg Valid Loss: 0.3290 / \n",
      "Epoch 15 Avg Train Loss: 0.3113 Avg Valid Loss: 0.3165 / \n",
      "Epoch 16 Avg Train Loss: 0.3105 Avg Valid Loss: 0.3285 / \n",
      "Epoch 17 Avg Train Loss: 0.3075 Avg Valid Loss: 0.3110 / \n",
      "Epoch 17 Save Best Valid Loss: 0.3110\n",
      "Epoch 18 Avg Train Loss: 0.3047 Avg Valid Loss: 0.3046 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3046\n",
      "Epoch 19 Avg Train Loss: 0.3012 Avg Valid Loss: 0.3058 / \n",
      "Epoch 20 Avg Train Loss: 0.3054 Avg Valid Loss: 0.3111 / \n",
      "Epoch 21 Avg Train Loss: 0.3050 Avg Valid Loss: 0.3212 / \n",
      "Epoch 22 Avg Train Loss: 0.3087 Avg Valid Loss: 0.3058 / \n",
      "Epoch 23 Avg Train Loss: 0.2999 Avg Valid Loss: 0.3150 / \n",
      "Epoch 24 Avg Train Loss: 0.3046 Avg Valid Loss: 0.3190 / \n",
      "Epoch 25 Avg Train Loss: 0.2949 Avg Valid Loss: 0.3134 / \n",
      "Epoch 26 Avg Train Loss: 0.3019 Avg Valid Loss: 0.3102 / \n",
      "Epoch 27 Avg Train Loss: 0.2994 Avg Valid Loss: 0.3161 / \n",
      "Epoch 28 Avg Train Loss: 0.2945 Avg Valid Loss: 0.3136 / \n",
      "Epoch 29 Avg Train Loss: 0.2927 Avg Valid Loss: 0.3269 / \n",
      "Epoch 30 Avg Train Loss: 0.2987 Avg Valid Loss: 0.3195 / \n",
      "Epoch 31 Avg Train Loss: 0.2978 Avg Valid Loss: 0.3102 / \n",
      "Epoch 32 Avg Train Loss: 0.2915 Avg Valid Loss: 0.3177 / \n",
      "Epoch 33 Avg Train Loss: 0.2951 Avg Valid Loss: 0.3473 / \n",
      "Epoch 34 Avg Train Loss: 0.2954 Avg Valid Loss: 0.3353 / \n",
      "Epoch 35 Avg Train Loss: 0.2939 Avg Valid Loss: 0.3163 / \n",
      "Epoch 36 Avg Train Loss: 0.2881 Avg Valid Loss: 0.3120 / \n",
      "Epoch 37 Avg Train Loss: 0.2949 Avg Valid Loss: 0.3109 / \n",
      "Epoch 38 Avg Train Loss: 0.2894 Avg Valid Loss: 0.3249 / \n",
      "Epoch 39 Avg Train Loss: 0.2893 Avg Valid Loss: 0.3177 / \n",
      "Epoch 40 Avg Train Loss: 0.2811 Avg Valid Loss: 0.3090 / \n",
      "Epoch 41 Avg Train Loss: 0.2878 Avg Valid Loss: 0.3124 / \n",
      "Epoch 42 Avg Train Loss: 0.2908 Avg Valid Loss: 0.3121 / \n",
      "Epoch 43 Avg Train Loss: 0.2879 Avg Valid Loss: 0.3046 / \n",
      "Epoch 43 Save Best Valid Loss: 0.3046\n",
      "Epoch 44 Avg Train Loss: 0.2910 Avg Valid Loss: 0.3297 / \n",
      "Epoch 45 Avg Train Loss: 0.2876 Avg Valid Loss: 0.3288 / \n",
      "Epoch 46 Avg Train Loss: 0.2888 Avg Valid Loss: 0.3348 / \n",
      "Epoch 47 Avg Train Loss: 0.2864 Avg Valid Loss: 0.3401 / \n",
      "Epoch 48 Avg Train Loss: 0.2805 Avg Valid Loss: 0.3204 / \n",
      "Epoch 49 Avg Train Loss: 0.2866 Avg Valid Loss: 0.3247 / \n",
      "Epoch 50 Avg Train Loss: 0.2786 Avg Valid Loss: 0.3165 / \n",
      "Epoch 51 Avg Train Loss: 0.2918 Avg Valid Loss: 0.3127 / \n",
      "Epoch 52 Avg Train Loss: 0.2886 Avg Valid Loss: 0.3081 / \n",
      "Epoch 53 Avg Train Loss: 0.2841 Avg Valid Loss: 0.3246 / \n",
      "Epoch 54 Avg Train Loss: 0.2847 Avg Valid Loss: 0.3249 / \n",
      "Epoch 55 Avg Train Loss: 0.2819 Avg Valid Loss: 0.3237 / \n",
      "Epoch 56 Avg Train Loss: 0.2821 Avg Valid Loss: 0.3289 / \n",
      "Epoch 57 Avg Train Loss: 0.2860 Avg Valid Loss: 0.3225 / \n",
      "Epoch 58 Avg Train Loss: 0.2767 Avg Valid Loss: 0.3163 / \n",
      "Epoch 59 Avg Train Loss: 0.2778 Avg Valid Loss: 0.3155 / \n",
      "Epoch 60 Avg Train Loss: 0.2881 Avg Valid Loss: 0.3085 / \n",
      "Epoch 61 Avg Train Loss: 0.2818 Avg Valid Loss: 0.3180 / \n",
      "Epoch 62 Avg Train Loss: 0.2811 Avg Valid Loss: 0.3063 / \n",
      "Epoch 63 Avg Train Loss: 0.2806 Avg Valid Loss: 0.3071 / \n",
      "Epoch 64 Avg Train Loss: 0.2778 Avg Valid Loss: 0.3070 / \n",
      "Epoch 65 Avg Train Loss: 0.2830 Avg Valid Loss: 0.3126 / \n",
      "Epoch 66 Avg Train Loss: 0.2774 Avg Valid Loss: 0.3496 / \n",
      "Epoch 67 Avg Train Loss: 0.2768 Avg Valid Loss: 0.3190 / \n",
      "Epoch 68 Avg Train Loss: 0.2788 Avg Valid Loss: 0.3445 / \n",
      "Epoch 69 Avg Train Loss: 0.2810 Avg Valid Loss: 0.3096 / \n",
      "Epoch 70 Avg Train Loss: 0.2778 Avg Valid Loss: 0.3570 / \n",
      "Epoch 71 Avg Train Loss: 0.2749 Avg Valid Loss: 0.3242 / \n",
      "Epoch 72 Avg Train Loss: 0.2770 Avg Valid Loss: 0.3150 / \n",
      "Epoch 73 Avg Train Loss: 0.2788 Avg Valid Loss: 0.3262 / \n",
      "Epoch 74 Avg Train Loss: 0.2737 Avg Valid Loss: 0.3174 / \n",
      "Epoch 75 Avg Train Loss: 0.2753 Avg Valid Loss: 0.3230 / \n",
      "Epoch 76 Avg Train Loss: 0.2786 Avg Valid Loss: 0.3188 / \n",
      "Epoch 77 Avg Train Loss: 0.2770 Avg Valid Loss: 0.3179 / \n",
      "Epoch 78 Avg Train Loss: 0.2767 Avg Valid Loss: 0.3121 / \n",
      "Epoch 79 Avg Train Loss: 0.2821 Avg Valid Loss: 0.3053 / \n",
      "Epoch 80 Avg Train Loss: 0.2824 Avg Valid Loss: 0.3043 / \n",
      "Epoch 80 Save Best Valid Loss: 0.3043\n",
      "Epoch 81 Avg Train Loss: 0.2746 Avg Valid Loss: 0.3113 / \n",
      "Epoch 82 Avg Train Loss: 0.2760 Avg Valid Loss: 0.3098 / \n",
      "Epoch 83 Avg Train Loss: 0.2732 Avg Valid Loss: 0.3145 / \n",
      "Epoch 84 Avg Train Loss: 0.2658 Avg Valid Loss: 0.3096 / \n",
      "Epoch 85 Avg Train Loss: 0.2675 Avg Valid Loss: 0.3178 / \n",
      "Epoch 86 Avg Train Loss: 0.2722 Avg Valid Loss: 0.3155 / \n",
      "Epoch 87 Avg Train Loss: 0.2701 Avg Valid Loss: 0.3156 / \n",
      "Epoch 88 Avg Train Loss: 0.2685 Avg Valid Loss: 0.3159 / \n",
      "Epoch 89 Avg Train Loss: 0.2698 Avg Valid Loss: 0.3158 / \n",
      "Epoch 90 Avg Train Loss: 0.2695 Avg Valid Loss: 0.3250 / \n",
      "Epoch 91 Avg Train Loss: 0.2705 Avg Valid Loss: 0.3171 / \n",
      "Epoch 92 Avg Train Loss: 0.2703 Avg Valid Loss: 0.3130 / \n",
      "Epoch 93 Avg Train Loss: 0.2714 Avg Valid Loss: 0.3086 / \n",
      "Epoch 94 Avg Train Loss: 0.2705 Avg Valid Loss: 0.3149 / \n",
      "Epoch 95 Avg Train Loss: 0.2649 Avg Valid Loss: 0.3226 / \n",
      "Epoch 96 Avg Train Loss: 0.2659 Avg Valid Loss: 0.3102 / \n",
      "Epoch 97 Avg Train Loss: 0.2719 Avg Valid Loss: 0.3099 / \n",
      "Epoch 98 Avg Train Loss: 0.2712 Avg Valid Loss: 0.3154 / \n",
      "Epoch 99 Avg Train Loss: 0.2611 Avg Valid Loss: 0.3133 / \n",
      "Epoch 100 Avg Train Loss: 0.2716 Avg Valid Loss: 0.3152 / \n",
      "========== fold: 1 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3043\n",
      "========== stage: 1 fold: 2 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5575 Avg Valid Loss: 0.5572 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5572\n",
      "Epoch 2 Avg Train Loss: 0.4850 Avg Valid Loss: 0.4566 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4566\n",
      "Epoch 3 Avg Train Loss: 0.4524 Avg Valid Loss: 0.5138 / \n",
      "Epoch 4 Avg Train Loss: 0.4414 Avg Valid Loss: 0.4387 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4387\n",
      "Epoch 5 Avg Train Loss: 0.4237 Avg Valid Loss: 0.4115 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4115\n",
      "Epoch 6 Avg Train Loss: 0.4185 Avg Valid Loss: 0.4089 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4089\n",
      "Epoch 7 Avg Train Loss: 0.4068 Avg Valid Loss: 0.3896 / \n",
      "Epoch 7 Save Best Valid Loss: 0.3896\n",
      "Epoch 8 Avg Train Loss: 0.4046 Avg Valid Loss: 0.4092 / \n",
      "Epoch 9 Avg Train Loss: 0.4010 Avg Valid Loss: 0.3971 / \n",
      "Epoch 10 Avg Train Loss: 0.3938 Avg Valid Loss: 0.3865 / \n",
      "Epoch 10 Save Best Valid Loss: 0.3865\n",
      "Epoch 11 Avg Train Loss: 0.3925 Avg Valid Loss: 0.3983 / \n",
      "Epoch 12 Avg Train Loss: 0.3845 Avg Valid Loss: 0.3985 / \n",
      "Epoch 13 Avg Train Loss: 0.3875 Avg Valid Loss: 0.3932 / \n",
      "Epoch 14 Avg Train Loss: 0.3853 Avg Valid Loss: 0.4066 / \n",
      "Epoch 15 Avg Train Loss: 0.3811 Avg Valid Loss: 0.3827 / \n",
      "Epoch 15 Save Best Valid Loss: 0.3827\n",
      "Epoch 16 Avg Train Loss: 0.3771 Avg Valid Loss: 0.3720 / \n",
      "Epoch 16 Save Best Valid Loss: 0.3720\n",
      "Epoch 17 Avg Train Loss: 0.3774 Avg Valid Loss: 0.3821 / \n",
      "Epoch 18 Avg Train Loss: 0.3790 Avg Valid Loss: 0.3672 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3672\n",
      "Epoch 19 Avg Train Loss: 0.3736 Avg Valid Loss: 0.3727 / \n",
      "Epoch 20 Avg Train Loss: 0.3711 Avg Valid Loss: 0.3859 / \n",
      "Epoch 21 Avg Train Loss: 0.3658 Avg Valid Loss: 0.3893 / \n",
      "Epoch 22 Avg Train Loss: 0.3662 Avg Valid Loss: 0.3701 / \n",
      "Epoch 23 Avg Train Loss: 0.3664 Avg Valid Loss: 0.3817 / \n",
      "Epoch 24 Avg Train Loss: 0.3672 Avg Valid Loss: 0.3705 / \n",
      "Epoch 25 Avg Train Loss: 0.3667 Avg Valid Loss: 0.3759 / \n",
      "Epoch 26 Avg Train Loss: 0.3642 Avg Valid Loss: 0.3720 / \n",
      "Epoch 27 Avg Train Loss: 0.3606 Avg Valid Loss: 0.3768 / \n",
      "Epoch 28 Avg Train Loss: 0.3592 Avg Valid Loss: 0.4027 / \n",
      "Epoch 29 Avg Train Loss: 0.3636 Avg Valid Loss: 0.3744 / \n",
      "Epoch 30 Avg Train Loss: 0.3593 Avg Valid Loss: 0.3758 / \n",
      "Epoch 31 Avg Train Loss: 0.3584 Avg Valid Loss: 0.3727 / \n",
      "Epoch 32 Avg Train Loss: 0.3599 Avg Valid Loss: 0.3838 / \n",
      "Epoch 33 Avg Train Loss: 0.3562 Avg Valid Loss: 0.3607 / \n",
      "Epoch 33 Save Best Valid Loss: 0.3607\n",
      "Epoch 34 Avg Train Loss: 0.3620 Avg Valid Loss: 0.3839 / \n",
      "Epoch 35 Avg Train Loss: 0.3537 Avg Valid Loss: 0.3623 / \n",
      "Epoch 36 Avg Train Loss: 0.3553 Avg Valid Loss: 0.3755 / \n",
      "Epoch 37 Avg Train Loss: 0.3529 Avg Valid Loss: 0.3552 / \n",
      "Epoch 37 Save Best Valid Loss: 0.3552\n",
      "Epoch 38 Avg Train Loss: 0.3465 Avg Valid Loss: 0.3733 / \n",
      "Epoch 39 Avg Train Loss: 0.3522 Avg Valid Loss: 0.3715 / \n",
      "Epoch 40 Avg Train Loss: 0.3551 Avg Valid Loss: 0.3641 / \n",
      "Epoch 41 Avg Train Loss: 0.3502 Avg Valid Loss: 0.3611 / \n",
      "Epoch 42 Avg Train Loss: 0.3511 Avg Valid Loss: 0.3632 / \n",
      "Epoch 43 Avg Train Loss: 0.3537 Avg Valid Loss: 0.3512 / \n",
      "Epoch 43 Save Best Valid Loss: 0.3512\n",
      "Epoch 44 Avg Train Loss: 0.3467 Avg Valid Loss: 0.3723 / \n",
      "Epoch 45 Avg Train Loss: 0.3450 Avg Valid Loss: 0.3616 / \n",
      "Epoch 46 Avg Train Loss: 0.3483 Avg Valid Loss: 0.3805 / \n",
      "Epoch 47 Avg Train Loss: 0.3497 Avg Valid Loss: 0.3562 / \n",
      "Epoch 48 Avg Train Loss: 0.3434 Avg Valid Loss: 0.3670 / \n",
      "Epoch 49 Avg Train Loss: 0.3470 Avg Valid Loss: 0.3451 / \n",
      "Epoch 49 Save Best Valid Loss: 0.3451\n",
      "Epoch 50 Avg Train Loss: 0.3463 Avg Valid Loss: 0.3891 / \n",
      "========== fold: 2 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3451\n",
      "========== stage: 2 fold: 2 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3854 Avg Valid Loss: 0.3337 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3337\n",
      "Epoch 2 Avg Train Loss: 0.3610 Avg Valid Loss: 0.3209 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3209\n",
      "Epoch 3 Avg Train Loss: 0.3435 Avg Valid Loss: 0.3182 / \n",
      "Epoch 3 Save Best Valid Loss: 0.3182\n",
      "Epoch 4 Avg Train Loss: 0.3379 Avg Valid Loss: 0.3147 / \n",
      "Epoch 4 Save Best Valid Loss: 0.3147\n",
      "Epoch 5 Avg Train Loss: 0.3344 Avg Valid Loss: 0.3143 / \n",
      "Epoch 5 Save Best Valid Loss: 0.3143\n",
      "Epoch 6 Avg Train Loss: 0.3277 Avg Valid Loss: 0.3279 / \n",
      "Epoch 7 Avg Train Loss: 0.3296 Avg Valid Loss: 0.3837 / \n",
      "Epoch 8 Avg Train Loss: 0.3293 Avg Valid Loss: 0.3538 / \n",
      "Epoch 9 Avg Train Loss: 0.3245 Avg Valid Loss: 0.3123 / \n",
      "Epoch 9 Save Best Valid Loss: 0.3123\n",
      "Epoch 10 Avg Train Loss: 0.3190 Avg Valid Loss: 0.3274 / \n",
      "Epoch 11 Avg Train Loss: 0.3280 Avg Valid Loss: 0.3158 / \n",
      "Epoch 12 Avg Train Loss: 0.3160 Avg Valid Loss: 0.3181 / \n",
      "Epoch 13 Avg Train Loss: 0.3214 Avg Valid Loss: 0.3107 / \n",
      "Epoch 13 Save Best Valid Loss: 0.3107\n",
      "Epoch 14 Avg Train Loss: 0.3151 Avg Valid Loss: 0.3169 / \n",
      "Epoch 15 Avg Train Loss: 0.3206 Avg Valid Loss: 0.3199 / \n",
      "Epoch 16 Avg Train Loss: 0.3157 Avg Valid Loss: 0.3199 / \n",
      "Epoch 17 Avg Train Loss: 0.3171 Avg Valid Loss: 0.3094 / \n",
      "Epoch 17 Save Best Valid Loss: 0.3094\n",
      "Epoch 18 Avg Train Loss: 0.3193 Avg Valid Loss: 0.3248 / \n",
      "Epoch 19 Avg Train Loss: 0.3048 Avg Valid Loss: 0.3045 / \n",
      "Epoch 19 Save Best Valid Loss: 0.3045\n",
      "Epoch 20 Avg Train Loss: 0.3127 Avg Valid Loss: 0.3006 / \n",
      "Epoch 20 Save Best Valid Loss: 0.3006\n",
      "Epoch 21 Avg Train Loss: 0.3166 Avg Valid Loss: 0.3054 / \n",
      "Epoch 22 Avg Train Loss: 0.3091 Avg Valid Loss: 0.2981 / \n",
      "Epoch 22 Save Best Valid Loss: 0.2981\n",
      "Epoch 23 Avg Train Loss: 0.3154 Avg Valid Loss: 0.3114 / \n",
      "Epoch 24 Avg Train Loss: 0.3120 Avg Valid Loss: 0.3113 / \n",
      "Epoch 25 Avg Train Loss: 0.3115 Avg Valid Loss: 0.3167 / \n",
      "Epoch 26 Avg Train Loss: 0.3032 Avg Valid Loss: 0.3104 / \n",
      "Epoch 27 Avg Train Loss: 0.3003 Avg Valid Loss: 0.3293 / \n",
      "Epoch 28 Avg Train Loss: 0.3055 Avg Valid Loss: 0.3055 / \n",
      "Epoch 29 Avg Train Loss: 0.3098 Avg Valid Loss: 0.3267 / \n",
      "Epoch 30 Avg Train Loss: 0.3043 Avg Valid Loss: 0.3225 / \n",
      "Epoch 31 Avg Train Loss: 0.2996 Avg Valid Loss: 0.3113 / \n",
      "Epoch 32 Avg Train Loss: 0.2998 Avg Valid Loss: 0.3142 / \n",
      "Epoch 33 Avg Train Loss: 0.2995 Avg Valid Loss: 0.3090 / \n",
      "Epoch 34 Avg Train Loss: 0.3061 Avg Valid Loss: 0.3432 / \n",
      "Epoch 35 Avg Train Loss: 0.3016 Avg Valid Loss: 0.3196 / \n",
      "Epoch 36 Avg Train Loss: 0.2986 Avg Valid Loss: 0.3269 / \n",
      "Epoch 37 Avg Train Loss: 0.2983 Avg Valid Loss: 0.3179 / \n",
      "Epoch 38 Avg Train Loss: 0.2934 Avg Valid Loss: 0.3061 / \n",
      "Epoch 39 Avg Train Loss: 0.2913 Avg Valid Loss: 0.3086 / \n",
      "Epoch 40 Avg Train Loss: 0.2998 Avg Valid Loss: 0.3208 / \n",
      "Epoch 41 Avg Train Loss: 0.3029 Avg Valid Loss: 0.2983 / \n",
      "Epoch 42 Avg Train Loss: 0.2947 Avg Valid Loss: 0.2948 / \n",
      "Epoch 42 Save Best Valid Loss: 0.2948\n",
      "Epoch 43 Avg Train Loss: 0.2943 Avg Valid Loss: 0.3068 / \n",
      "Epoch 44 Avg Train Loss: 0.2955 Avg Valid Loss: 0.3323 / \n",
      "Epoch 45 Avg Train Loss: 0.3016 Avg Valid Loss: 0.3172 / \n",
      "Epoch 46 Avg Train Loss: 0.2951 Avg Valid Loss: 0.3176 / \n",
      "Epoch 47 Avg Train Loss: 0.2945 Avg Valid Loss: 0.3197 / \n",
      "Epoch 48 Avg Train Loss: 0.2890 Avg Valid Loss: 0.3266 / \n",
      "Epoch 49 Avg Train Loss: 0.2936 Avg Valid Loss: 0.3261 / \n",
      "Epoch 50 Avg Train Loss: 0.2837 Avg Valid Loss: 0.3202 / \n",
      "Epoch 51 Avg Train Loss: 0.2947 Avg Valid Loss: 0.3298 / \n",
      "Epoch 52 Avg Train Loss: 0.2899 Avg Valid Loss: 0.3131 / \n",
      "Epoch 53 Avg Train Loss: 0.2895 Avg Valid Loss: 0.3398 / \n",
      "Epoch 54 Avg Train Loss: 0.2885 Avg Valid Loss: 0.3074 / \n",
      "Epoch 55 Avg Train Loss: 0.2884 Avg Valid Loss: 0.3230 / \n",
      "Epoch 56 Avg Train Loss: 0.2964 Avg Valid Loss: 0.3083 / \n",
      "Epoch 57 Avg Train Loss: 0.2903 Avg Valid Loss: 0.3248 / \n",
      "Epoch 58 Avg Train Loss: 0.2930 Avg Valid Loss: 0.3108 / \n",
      "Epoch 59 Avg Train Loss: 0.2945 Avg Valid Loss: 0.3266 / \n",
      "Epoch 60 Avg Train Loss: 0.2910 Avg Valid Loss: 0.3238 / \n",
      "Epoch 61 Avg Train Loss: 0.2870 Avg Valid Loss: 0.3159 / \n",
      "Epoch 62 Avg Train Loss: 0.2913 Avg Valid Loss: 0.3194 / \n",
      "Epoch 63 Avg Train Loss: 0.2886 Avg Valid Loss: 0.3024 / \n",
      "Epoch 64 Avg Train Loss: 0.2908 Avg Valid Loss: 0.3153 / \n",
      "Epoch 65 Avg Train Loss: 0.2920 Avg Valid Loss: 0.3083 / \n",
      "Epoch 66 Avg Train Loss: 0.2856 Avg Valid Loss: 0.3050 / \n",
      "Epoch 67 Avg Train Loss: 0.2792 Avg Valid Loss: 0.3044 / \n",
      "Epoch 68 Avg Train Loss: 0.2780 Avg Valid Loss: 0.3051 / \n",
      "Epoch 69 Avg Train Loss: 0.2789 Avg Valid Loss: 0.3060 / \n",
      "Epoch 70 Avg Train Loss: 0.2830 Avg Valid Loss: 0.3071 / \n",
      "Epoch 71 Avg Train Loss: 0.2826 Avg Valid Loss: 0.3069 / \n",
      "Epoch 72 Avg Train Loss: 0.2883 Avg Valid Loss: 0.3060 / \n",
      "Epoch 73 Avg Train Loss: 0.2795 Avg Valid Loss: 0.3061 / \n",
      "Epoch 74 Avg Train Loss: 0.2828 Avg Valid Loss: 0.3059 / \n",
      "Epoch 75 Avg Train Loss: 0.2819 Avg Valid Loss: 0.3026 / \n",
      "Epoch 76 Avg Train Loss: 0.2838 Avg Valid Loss: 0.3045 / \n",
      "Epoch 77 Avg Train Loss: 0.2839 Avg Valid Loss: 0.3044 / \n",
      "Epoch 78 Avg Train Loss: 0.2809 Avg Valid Loss: 0.3069 / \n",
      "Epoch 79 Avg Train Loss: 0.2808 Avg Valid Loss: 0.3046 / \n",
      "Epoch 80 Avg Train Loss: 0.2804 Avg Valid Loss: 0.3049 / \n",
      "Epoch 81 Avg Train Loss: 0.2797 Avg Valid Loss: 0.3078 / \n",
      "Epoch 82 Avg Train Loss: 0.2836 Avg Valid Loss: 0.3075 / \n",
      "Epoch 83 Avg Train Loss: 0.2840 Avg Valid Loss: 0.3038 / \n",
      "Epoch 84 Avg Train Loss: 0.2886 Avg Valid Loss: 0.3077 / \n",
      "Epoch 85 Avg Train Loss: 0.2826 Avg Valid Loss: 0.3068 / \n",
      "Epoch 86 Avg Train Loss: 0.2792 Avg Valid Loss: 0.3034 / \n",
      "Epoch 87 Avg Train Loss: 0.2888 Avg Valid Loss: 0.3043 / \n",
      "Epoch 88 Avg Train Loss: 0.2791 Avg Valid Loss: 0.3086 / \n",
      "Epoch 89 Avg Train Loss: 0.2820 Avg Valid Loss: 0.3052 / \n",
      "Epoch 90 Avg Train Loss: 0.2831 Avg Valid Loss: 0.3050 / \n",
      "Epoch 91 Avg Train Loss: 0.2848 Avg Valid Loss: 0.3040 / \n",
      "Epoch 92 Avg Train Loss: 0.2800 Avg Valid Loss: 0.3061 / \n",
      "Epoch 93 Avg Train Loss: 0.2911 Avg Valid Loss: 0.3070 / \n",
      "Epoch 94 Avg Train Loss: 0.2811 Avg Valid Loss: 0.3060 / \n",
      "Epoch 95 Avg Train Loss: 0.2825 Avg Valid Loss: 0.3061 / \n",
      "Epoch 96 Avg Train Loss: 0.2817 Avg Valid Loss: 0.3057 / \n",
      "Epoch 97 Avg Train Loss: 0.2829 Avg Valid Loss: 0.3071 / \n",
      "Epoch 98 Avg Train Loss: 0.2801 Avg Valid Loss: 0.3051 / \n",
      "Epoch 99 Avg Train Loss: 0.2814 Avg Valid Loss: 0.3033 / \n",
      "Epoch 100 Avg Train Loss: 0.2832 Avg Valid Loss: 0.3034 / \n",
      "========== fold: 2 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.2948\n",
      "========== stage: 1 fold: 3 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5563 Avg Valid Loss: 0.5064 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5064\n",
      "Epoch 2 Avg Train Loss: 0.4692 Avg Valid Loss: 0.4366 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4366\n",
      "Epoch 3 Avg Train Loss: 0.4461 Avg Valid Loss: 0.4686 / \n",
      "Epoch 4 Avg Train Loss: 0.4276 Avg Valid Loss: 0.4351 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4351\n",
      "Epoch 5 Avg Train Loss: 0.4174 Avg Valid Loss: 0.4372 / \n",
      "Epoch 6 Avg Train Loss: 0.4132 Avg Valid Loss: 0.4060 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4060\n",
      "Epoch 7 Avg Train Loss: 0.4055 Avg Valid Loss: 0.3958 / \n",
      "Epoch 7 Save Best Valid Loss: 0.3958\n",
      "Epoch 8 Avg Train Loss: 0.4017 Avg Valid Loss: 0.3897 / \n",
      "Epoch 8 Save Best Valid Loss: 0.3897\n",
      "Epoch 9 Avg Train Loss: 0.3951 Avg Valid Loss: 0.3994 / \n",
      "Epoch 10 Avg Train Loss: 0.3956 Avg Valid Loss: 0.3959 / \n",
      "Epoch 11 Avg Train Loss: 0.3871 Avg Valid Loss: 0.3908 / \n",
      "Epoch 12 Avg Train Loss: 0.3886 Avg Valid Loss: 0.3967 / \n",
      "Epoch 13 Avg Train Loss: 0.3860 Avg Valid Loss: 0.3908 / \n",
      "Epoch 14 Avg Train Loss: 0.3838 Avg Valid Loss: 0.3944 / \n",
      "Epoch 15 Avg Train Loss: 0.3814 Avg Valid Loss: 0.3935 / \n",
      "Epoch 16 Avg Train Loss: 0.3793 Avg Valid Loss: 0.3747 / \n",
      "Epoch 16 Save Best Valid Loss: 0.3747\n",
      "Epoch 17 Avg Train Loss: 0.3805 Avg Valid Loss: 0.3850 / \n",
      "Epoch 18 Avg Train Loss: 0.3779 Avg Valid Loss: 0.3746 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3746\n",
      "Epoch 19 Avg Train Loss: 0.3726 Avg Valid Loss: 0.3747 / \n",
      "Epoch 20 Avg Train Loss: 0.3710 Avg Valid Loss: 0.3705 / \n",
      "Epoch 20 Save Best Valid Loss: 0.3705\n",
      "Epoch 21 Avg Train Loss: 0.3695 Avg Valid Loss: 0.4027 / \n",
      "Epoch 22 Avg Train Loss: 0.3686 Avg Valid Loss: 0.3744 / \n",
      "Epoch 23 Avg Train Loss: 0.3649 Avg Valid Loss: 0.3905 / \n",
      "Epoch 24 Avg Train Loss: 0.3638 Avg Valid Loss: 0.3795 / \n",
      "Epoch 25 Avg Train Loss: 0.3644 Avg Valid Loss: 0.3886 / \n",
      "Epoch 26 Avg Train Loss: 0.3623 Avg Valid Loss: 0.3743 / \n",
      "Epoch 27 Avg Train Loss: 0.3632 Avg Valid Loss: 0.3626 / \n",
      "Epoch 27 Save Best Valid Loss: 0.3626\n",
      "Epoch 28 Avg Train Loss: 0.3576 Avg Valid Loss: 0.3603 / \n",
      "Epoch 28 Save Best Valid Loss: 0.3603\n",
      "Epoch 29 Avg Train Loss: 0.3625 Avg Valid Loss: 0.3675 / \n",
      "Epoch 30 Avg Train Loss: 0.3574 Avg Valid Loss: 0.3608 / \n",
      "Epoch 31 Avg Train Loss: 0.3560 Avg Valid Loss: 0.3661 / \n",
      "Epoch 32 Avg Train Loss: 0.3588 Avg Valid Loss: 0.3763 / \n",
      "Epoch 33 Avg Train Loss: 0.3541 Avg Valid Loss: 0.3615 / \n",
      "Epoch 34 Avg Train Loss: 0.3550 Avg Valid Loss: 0.3664 / \n",
      "Epoch 35 Avg Train Loss: 0.3544 Avg Valid Loss: 0.3503 / \n",
      "Epoch 35 Save Best Valid Loss: 0.3503\n",
      "Epoch 36 Avg Train Loss: 0.3553 Avg Valid Loss: 0.3665 / \n",
      "Epoch 37 Avg Train Loss: 0.3502 Avg Valid Loss: 0.3606 / \n",
      "Epoch 38 Avg Train Loss: 0.3517 Avg Valid Loss: 0.3546 / \n",
      "Epoch 39 Avg Train Loss: 0.3484 Avg Valid Loss: 0.3788 / \n",
      "Epoch 40 Avg Train Loss: 0.3523 Avg Valid Loss: 0.3875 / \n",
      "Epoch 41 Avg Train Loss: 0.3464 Avg Valid Loss: 0.3599 / \n",
      "Epoch 42 Avg Train Loss: 0.3494 Avg Valid Loss: 0.3570 / \n",
      "Epoch 43 Avg Train Loss: 0.3530 Avg Valid Loss: 0.3669 / \n",
      "Epoch 44 Avg Train Loss: 0.3495 Avg Valid Loss: 0.3741 / \n",
      "Epoch 45 Avg Train Loss: 0.3505 Avg Valid Loss: 0.3649 / \n",
      "Epoch 46 Avg Train Loss: 0.3458 Avg Valid Loss: 0.3632 / \n",
      "Epoch 47 Avg Train Loss: 0.3500 Avg Valid Loss: 0.3627 / \n",
      "Epoch 48 Avg Train Loss: 0.3471 Avg Valid Loss: 0.3729 / \n",
      "Epoch 49 Avg Train Loss: 0.3423 Avg Valid Loss: 0.3491 / \n",
      "Epoch 49 Save Best Valid Loss: 0.3491\n",
      "Epoch 50 Avg Train Loss: 0.3444 Avg Valid Loss: 0.3652 / \n",
      "========== fold: 3 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3491\n",
      "========== stage: 2 fold: 3 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3833 Avg Valid Loss: 0.3269 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3269\n",
      "Epoch 2 Avg Train Loss: 0.3509 Avg Valid Loss: 0.3210 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3210\n",
      "Epoch 3 Avg Train Loss: 0.3472 Avg Valid Loss: 0.3336 / \n",
      "Epoch 4 Avg Train Loss: 0.3447 Avg Valid Loss: 0.3253 / \n",
      "Epoch 5 Avg Train Loss: 0.3360 Avg Valid Loss: 0.3496 / \n",
      "Epoch 6 Avg Train Loss: 0.3389 Avg Valid Loss: 0.3508 / \n",
      "Epoch 7 Avg Train Loss: 0.3366 Avg Valid Loss: 0.3503 / \n",
      "Epoch 8 Avg Train Loss: 0.3281 Avg Valid Loss: 0.3533 / \n",
      "Epoch 9 Avg Train Loss: 0.3284 Avg Valid Loss: 0.3324 / \n",
      "Epoch 10 Avg Train Loss: 0.3297 Avg Valid Loss: 0.3443 / \n",
      "Epoch 11 Avg Train Loss: 0.3218 Avg Valid Loss: 0.3298 / \n",
      "Epoch 12 Avg Train Loss: 0.3181 Avg Valid Loss: 0.3251 / \n",
      "Epoch 13 Avg Train Loss: 0.3234 Avg Valid Loss: 0.3116 / \n",
      "Epoch 13 Save Best Valid Loss: 0.3116\n",
      "Epoch 14 Avg Train Loss: 0.3168 Avg Valid Loss: 0.3250 / \n",
      "Epoch 15 Avg Train Loss: 0.3128 Avg Valid Loss: 0.3346 / \n",
      "Epoch 16 Avg Train Loss: 0.3173 Avg Valid Loss: 0.3184 / \n",
      "Epoch 17 Avg Train Loss: 0.3163 Avg Valid Loss: 0.3167 / \n",
      "Epoch 18 Avg Train Loss: 0.3117 Avg Valid Loss: 0.3172 / \n",
      "Epoch 19 Avg Train Loss: 0.3135 Avg Valid Loss: 0.3251 / \n",
      "Epoch 20 Avg Train Loss: 0.3120 Avg Valid Loss: 0.3228 / \n",
      "Epoch 21 Avg Train Loss: 0.3066 Avg Valid Loss: 0.3110 / \n",
      "Epoch 21 Save Best Valid Loss: 0.3110\n",
      "Epoch 22 Avg Train Loss: 0.3119 Avg Valid Loss: 0.3239 / \n",
      "Epoch 23 Avg Train Loss: 0.3032 Avg Valid Loss: 0.3141 / \n",
      "Epoch 24 Avg Train Loss: 0.3054 Avg Valid Loss: 0.3298 / \n",
      "Epoch 25 Avg Train Loss: 0.3056 Avg Valid Loss: 0.3140 / \n",
      "Epoch 26 Avg Train Loss: 0.3043 Avg Valid Loss: 0.3658 / \n",
      "Epoch 27 Avg Train Loss: 0.3005 Avg Valid Loss: 0.3593 / \n",
      "Epoch 28 Avg Train Loss: 0.3090 Avg Valid Loss: 0.3340 / \n",
      "Epoch 29 Avg Train Loss: 0.3026 Avg Valid Loss: 0.3345 / \n",
      "Epoch 30 Avg Train Loss: 0.3044 Avg Valid Loss: 0.3406 / \n",
      "Epoch 31 Avg Train Loss: 0.3035 Avg Valid Loss: 0.3039 / \n",
      "Epoch 31 Save Best Valid Loss: 0.3039\n",
      "Epoch 32 Avg Train Loss: 0.2990 Avg Valid Loss: 0.3244 / \n",
      "Epoch 33 Avg Train Loss: 0.3037 Avg Valid Loss: 0.3240 / \n",
      "Epoch 34 Avg Train Loss: 0.2971 Avg Valid Loss: 0.3429 / \n",
      "Epoch 35 Avg Train Loss: 0.3066 Avg Valid Loss: 0.3406 / \n",
      "Epoch 36 Avg Train Loss: 0.2961 Avg Valid Loss: 0.3140 / \n",
      "Epoch 37 Avg Train Loss: 0.2984 Avg Valid Loss: 0.3359 / \n",
      "Epoch 38 Avg Train Loss: 0.2923 Avg Valid Loss: 0.3218 / \n",
      "Epoch 39 Avg Train Loss: 0.2996 Avg Valid Loss: 0.3151 / \n",
      "Epoch 40 Avg Train Loss: 0.2997 Avg Valid Loss: 0.3017 / \n",
      "Epoch 40 Save Best Valid Loss: 0.3017\n",
      "Epoch 41 Avg Train Loss: 0.2972 Avg Valid Loss: 0.3008 / \n",
      "Epoch 41 Save Best Valid Loss: 0.3008\n",
      "Epoch 42 Avg Train Loss: 0.2945 Avg Valid Loss: 0.3305 / \n",
      "Epoch 43 Avg Train Loss: 0.2916 Avg Valid Loss: 0.3245 / \n",
      "Epoch 44 Avg Train Loss: 0.2972 Avg Valid Loss: 0.3379 / \n",
      "Epoch 45 Avg Train Loss: 0.2968 Avg Valid Loss: 0.3915 / \n",
      "Epoch 46 Avg Train Loss: 0.2923 Avg Valid Loss: 0.3042 / \n",
      "Epoch 47 Avg Train Loss: 0.2956 Avg Valid Loss: 0.3164 / \n",
      "Epoch 48 Avg Train Loss: 0.2890 Avg Valid Loss: 0.3095 / \n",
      "Epoch 49 Avg Train Loss: 0.2935 Avg Valid Loss: 0.3163 / \n",
      "Epoch 50 Avg Train Loss: 0.2919 Avg Valid Loss: 0.3266 / \n",
      "Epoch 51 Avg Train Loss: 0.2964 Avg Valid Loss: 0.3090 / \n",
      "Epoch 52 Avg Train Loss: 0.2922 Avg Valid Loss: 0.3242 / \n",
      "Epoch 53 Avg Train Loss: 0.2937 Avg Valid Loss: 0.3256 / \n",
      "Epoch 54 Avg Train Loss: 0.2895 Avg Valid Loss: 0.3227 / \n",
      "Epoch 55 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3238 / \n",
      "Epoch 56 Avg Train Loss: 0.2918 Avg Valid Loss: 0.3278 / \n",
      "Epoch 57 Avg Train Loss: 0.2891 Avg Valid Loss: 0.3083 / \n",
      "Epoch 58 Avg Train Loss: 0.2901 Avg Valid Loss: 0.3096 / \n",
      "Epoch 59 Avg Train Loss: 0.2852 Avg Valid Loss: 0.3131 / \n",
      "Epoch 60 Avg Train Loss: 0.2901 Avg Valid Loss: 0.3441 / \n",
      "Epoch 61 Avg Train Loss: 0.2822 Avg Valid Loss: 0.3045 / \n",
      "Epoch 62 Avg Train Loss: 0.2864 Avg Valid Loss: 0.3344 / \n",
      "Epoch 63 Avg Train Loss: 0.2823 Avg Valid Loss: 0.3105 / \n",
      "Epoch 64 Avg Train Loss: 0.2842 Avg Valid Loss: 0.3114 / \n",
      "Epoch 65 Avg Train Loss: 0.2866 Avg Valid Loss: 0.3349 / \n",
      "Epoch 66 Avg Train Loss: 0.2891 Avg Valid Loss: 0.3159 / \n",
      "Epoch 67 Avg Train Loss: 0.2836 Avg Valid Loss: 0.3137 / \n",
      "Epoch 68 Avg Train Loss: 0.2782 Avg Valid Loss: 0.3155 / \n",
      "Epoch 69 Avg Train Loss: 0.2744 Avg Valid Loss: 0.3153 / \n",
      "Epoch 70 Avg Train Loss: 0.2723 Avg Valid Loss: 0.3191 / \n",
      "Epoch 71 Avg Train Loss: 0.2758 Avg Valid Loss: 0.3147 / \n",
      "Epoch 72 Avg Train Loss: 0.2796 Avg Valid Loss: 0.3157 / \n",
      "Epoch 73 Avg Train Loss: 0.2811 Avg Valid Loss: 0.3157 / \n",
      "Epoch 74 Avg Train Loss: 0.2743 Avg Valid Loss: 0.3197 / \n",
      "Epoch 75 Avg Train Loss: 0.2738 Avg Valid Loss: 0.3124 / \n",
      "Epoch 76 Avg Train Loss: 0.2716 Avg Valid Loss: 0.3191 / \n",
      "Epoch 77 Avg Train Loss: 0.2810 Avg Valid Loss: 0.3154 / \n",
      "Epoch 78 Avg Train Loss: 0.2797 Avg Valid Loss: 0.3149 / \n",
      "Epoch 79 Avg Train Loss: 0.2736 Avg Valid Loss: 0.3198 / \n",
      "Epoch 80 Avg Train Loss: 0.2815 Avg Valid Loss: 0.3179 / \n",
      "Epoch 81 Avg Train Loss: 0.2727 Avg Valid Loss: 0.3163 / \n",
      "Epoch 82 Avg Train Loss: 0.2687 Avg Valid Loss: 0.3155 / \n",
      "Epoch 83 Avg Train Loss: 0.2744 Avg Valid Loss: 0.3157 / \n",
      "Epoch 84 Avg Train Loss: 0.2769 Avg Valid Loss: 0.3187 / \n",
      "Epoch 85 Avg Train Loss: 0.2736 Avg Valid Loss: 0.3172 / \n",
      "Epoch 86 Avg Train Loss: 0.2785 Avg Valid Loss: 0.3180 / \n",
      "Epoch 87 Avg Train Loss: 0.2753 Avg Valid Loss: 0.3167 / \n",
      "Epoch 88 Avg Train Loss: 0.2797 Avg Valid Loss: 0.3145 / \n",
      "Epoch 89 Avg Train Loss: 0.2742 Avg Valid Loss: 0.3249 / \n",
      "Epoch 90 Avg Train Loss: 0.2746 Avg Valid Loss: 0.3170 / \n",
      "Epoch 91 Avg Train Loss: 0.2798 Avg Valid Loss: 0.3171 / \n",
      "Epoch 92 Avg Train Loss: 0.2820 Avg Valid Loss: 0.3146 / \n",
      "Epoch 93 Avg Train Loss: 0.2770 Avg Valid Loss: 0.3185 / \n",
      "Epoch 94 Avg Train Loss: 0.2726 Avg Valid Loss: 0.3175 / \n",
      "Epoch 95 Avg Train Loss: 0.2765 Avg Valid Loss: 0.3160 / \n",
      "Epoch 96 Avg Train Loss: 0.2758 Avg Valid Loss: 0.3178 / \n",
      "Epoch 97 Avg Train Loss: 0.2772 Avg Valid Loss: 0.3152 / \n",
      "Epoch 98 Avg Train Loss: 0.2765 Avg Valid Loss: 0.3178 / \n",
      "Epoch 99 Avg Train Loss: 0.2719 Avg Valid Loss: 0.3139 / \n",
      "Epoch 100 Avg Train Loss: 0.2756 Avg Valid Loss: 0.3163 / \n",
      "========== fold: 3 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3008\n",
      "========== stage: 1 fold: 4 training 169 / 22 ==========\n",
      "Epoch 1 Avg Train Loss: 0.5561 Avg Valid Loss: 0.4951 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4951\n",
      "Epoch 2 Avg Train Loss: 0.4679 Avg Valid Loss: 0.4268 / \n",
      "Epoch 2 Save Best Valid Loss: 0.4268\n",
      "Epoch 3 Avg Train Loss: 0.4364 Avg Valid Loss: 0.4081 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4081\n",
      "Epoch 4 Avg Train Loss: 0.4256 Avg Valid Loss: 0.4283 / \n",
      "Epoch 5 Avg Train Loss: 0.4124 Avg Valid Loss: 0.3946 / \n",
      "Epoch 5 Save Best Valid Loss: 0.3946\n",
      "Epoch 6 Avg Train Loss: 0.4081 Avg Valid Loss: 0.4289 / \n",
      "Epoch 7 Avg Train Loss: 0.3987 Avg Valid Loss: 0.4035 / \n",
      "Epoch 8 Avg Train Loss: 0.3988 Avg Valid Loss: 0.3962 / \n",
      "Epoch 9 Avg Train Loss: 0.3909 Avg Valid Loss: 0.3922 / \n",
      "Epoch 9 Save Best Valid Loss: 0.3922\n",
      "Epoch 10 Avg Train Loss: 0.3943 Avg Valid Loss: 0.3865 / \n",
      "Epoch 10 Save Best Valid Loss: 0.3865\n",
      "Epoch 11 Avg Train Loss: 0.3893 Avg Valid Loss: 0.3919 / \n",
      "Epoch 12 Avg Train Loss: 0.3866 Avg Valid Loss: 0.4325 / \n",
      "Epoch 13 Avg Train Loss: 0.3831 Avg Valid Loss: 0.3879 / \n",
      "Epoch 14 Avg Train Loss: 0.3756 Avg Valid Loss: 0.3925 / \n",
      "Epoch 15 Avg Train Loss: 0.3752 Avg Valid Loss: 0.3965 / \n",
      "Epoch 16 Avg Train Loss: 0.3726 Avg Valid Loss: 0.4019 / \n",
      "Epoch 17 Avg Train Loss: 0.3735 Avg Valid Loss: 0.3834 / \n",
      "Epoch 17 Save Best Valid Loss: 0.3834\n",
      "Epoch 18 Avg Train Loss: 0.3686 Avg Valid Loss: 0.3889 / \n",
      "Epoch 19 Avg Train Loss: 0.3694 Avg Valid Loss: 0.4012 / \n",
      "Epoch 20 Avg Train Loss: 0.3687 Avg Valid Loss: 0.3714 / \n",
      "Epoch 20 Save Best Valid Loss: 0.3714\n",
      "Epoch 21 Avg Train Loss: 0.3627 Avg Valid Loss: 0.4064 / \n",
      "Epoch 22 Avg Train Loss: 0.3610 Avg Valid Loss: 0.4010 / \n",
      "Epoch 23 Avg Train Loss: 0.3674 Avg Valid Loss: 0.3716 / \n",
      "Epoch 24 Avg Train Loss: 0.3632 Avg Valid Loss: 0.3886 / \n",
      "Epoch 25 Avg Train Loss: 0.3607 Avg Valid Loss: 0.3900 / \n",
      "Epoch 26 Avg Train Loss: 0.3592 Avg Valid Loss: 0.4010 / \n",
      "Epoch 27 Avg Train Loss: 0.3579 Avg Valid Loss: 0.4002 / \n",
      "Epoch 28 Avg Train Loss: 0.3601 Avg Valid Loss: 0.3882 / \n",
      "Epoch 29 Avg Train Loss: 0.3575 Avg Valid Loss: 0.3625 / \n",
      "Epoch 29 Save Best Valid Loss: 0.3625\n",
      "Epoch 30 Avg Train Loss: 0.3555 Avg Valid Loss: 0.4042 / \n",
      "Epoch 31 Avg Train Loss: 0.3526 Avg Valid Loss: 0.3801 / \n",
      "Epoch 32 Avg Train Loss: 0.3544 Avg Valid Loss: 0.4425 / \n",
      "Epoch 33 Avg Train Loss: 0.3503 Avg Valid Loss: 0.3947 / \n",
      "Epoch 34 Avg Train Loss: 0.3552 Avg Valid Loss: 0.3702 / \n",
      "Epoch 35 Avg Train Loss: 0.3507 Avg Valid Loss: 0.3906 / \n",
      "Epoch 36 Avg Train Loss: 0.3513 Avg Valid Loss: 0.3858 / \n",
      "Epoch 37 Avg Train Loss: 0.3517 Avg Valid Loss: 0.3875 / \n",
      "Epoch 38 Avg Train Loss: 0.3503 Avg Valid Loss: 0.3613 / \n",
      "Epoch 38 Save Best Valid Loss: 0.3613\n",
      "Epoch 39 Avg Train Loss: 0.3430 Avg Valid Loss: 0.4034 / \n",
      "Epoch 40 Avg Train Loss: 0.3477 Avg Valid Loss: 0.3891 / \n",
      "Epoch 41 Avg Train Loss: 0.3459 Avg Valid Loss: 0.3899 / \n",
      "Epoch 42 Avg Train Loss: 0.3480 Avg Valid Loss: 0.3589 / \n",
      "Epoch 42 Save Best Valid Loss: 0.3589\n",
      "Epoch 43 Avg Train Loss: 0.3456 Avg Valid Loss: 0.4001 / \n",
      "Epoch 44 Avg Train Loss: 0.3426 Avg Valid Loss: 0.3919 / \n",
      "Epoch 45 Avg Train Loss: 0.3447 Avg Valid Loss: 0.3862 / \n",
      "Epoch 46 Avg Train Loss: 0.3430 Avg Valid Loss: 0.3745 / \n",
      "Epoch 47 Avg Train Loss: 0.3445 Avg Valid Loss: 0.3827 / \n",
      "Epoch 48 Avg Train Loss: 0.3389 Avg Valid Loss: 0.3904 / \n",
      "Epoch 49 Avg Train Loss: 0.3394 Avg Valid Loss: 0.3821 / \n",
      "Epoch 50 Avg Train Loss: 0.3416 Avg Valid Loss: 0.3735 / \n",
      "========== fold: 4 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3589\n",
      "========== stage: 2 fold: 4 training 81 / 11 ==========\n",
      "Epoch 1 Avg Train Loss: 0.3890 Avg Valid Loss: 0.3398 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3398\n",
      "Epoch 2 Avg Train Loss: 0.3558 Avg Valid Loss: 0.3286 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3286\n",
      "Epoch 3 Avg Train Loss: 0.3470 Avg Valid Loss: 0.3333 / \n",
      "Epoch 4 Avg Train Loss: 0.3409 Avg Valid Loss: 0.3272 / \n",
      "Epoch 4 Save Best Valid Loss: 0.3272\n",
      "Epoch 5 Avg Train Loss: 0.3418 Avg Valid Loss: 0.3386 / \n",
      "Epoch 6 Avg Train Loss: 0.3364 Avg Valid Loss: 0.3371 / \n",
      "Epoch 7 Avg Train Loss: 0.3398 Avg Valid Loss: 0.3359 / \n",
      "Epoch 8 Avg Train Loss: 0.3272 Avg Valid Loss: 0.3342 / \n",
      "Epoch 9 Avg Train Loss: 0.3330 Avg Valid Loss: 0.3336 / \n",
      "Epoch 10 Avg Train Loss: 0.3291 Avg Valid Loss: 0.3621 / \n",
      "Epoch 11 Avg Train Loss: 0.3231 Avg Valid Loss: 0.3529 / \n",
      "Epoch 12 Avg Train Loss: 0.3298 Avg Valid Loss: 0.3408 / \n",
      "Epoch 13 Avg Train Loss: 0.3236 Avg Valid Loss: 0.3390 / \n",
      "Epoch 14 Avg Train Loss: 0.3255 Avg Valid Loss: 0.3400 / \n",
      "Epoch 15 Avg Train Loss: 0.3213 Avg Valid Loss: 0.3342 / \n",
      "Epoch 16 Avg Train Loss: 0.3144 Avg Valid Loss: 0.3347 / \n",
      "Epoch 17 Avg Train Loss: 0.3221 Avg Valid Loss: 0.3292 / \n",
      "Epoch 18 Avg Train Loss: 0.3197 Avg Valid Loss: 0.3188 / \n",
      "Epoch 18 Save Best Valid Loss: 0.3188\n",
      "Epoch 19 Avg Train Loss: 0.3175 Avg Valid Loss: 0.3194 / \n",
      "Epoch 20 Avg Train Loss: 0.3167 Avg Valid Loss: 0.3183 / \n",
      "Epoch 20 Save Best Valid Loss: 0.3183\n",
      "Epoch 21 Avg Train Loss: 0.3147 Avg Valid Loss: 0.3219 / \n",
      "Epoch 22 Avg Train Loss: 0.3056 Avg Valid Loss: 0.3280 / \n",
      "Epoch 23 Avg Train Loss: 0.3132 Avg Valid Loss: 0.3326 / \n",
      "Epoch 24 Avg Train Loss: 0.3087 Avg Valid Loss: 0.3303 / \n",
      "Epoch 25 Avg Train Loss: 0.3148 Avg Valid Loss: 0.3288 / \n",
      "Epoch 26 Avg Train Loss: 0.3015 Avg Valid Loss: 0.3424 / \n",
      "Epoch 27 Avg Train Loss: 0.3084 Avg Valid Loss: 0.3427 / \n",
      "Epoch 28 Avg Train Loss: 0.3122 Avg Valid Loss: 0.3398 / \n",
      "Epoch 29 Avg Train Loss: 0.3094 Avg Valid Loss: 0.3335 / \n",
      "Epoch 30 Avg Train Loss: 0.3052 Avg Valid Loss: 0.3671 / \n",
      "Epoch 31 Avg Train Loss: 0.3090 Avg Valid Loss: 0.3389 / \n",
      "Epoch 32 Avg Train Loss: 0.3056 Avg Valid Loss: 0.3333 / \n",
      "Epoch 33 Avg Train Loss: 0.3003 Avg Valid Loss: 0.3257 / \n",
      "Epoch 34 Avg Train Loss: 0.3053 Avg Valid Loss: 0.3161 / \n",
      "Epoch 34 Save Best Valid Loss: 0.3161\n",
      "Epoch 35 Avg Train Loss: 0.2995 Avg Valid Loss: 0.3232 / \n",
      "Epoch 36 Avg Train Loss: 0.2995 Avg Valid Loss: 0.3299 / \n",
      "Epoch 37 Avg Train Loss: 0.3094 Avg Valid Loss: 0.3170 / \n",
      "Epoch 38 Avg Train Loss: 0.2999 Avg Valid Loss: 0.3371 / \n",
      "Epoch 39 Avg Train Loss: 0.3043 Avg Valid Loss: 0.3109 / \n",
      "Epoch 39 Save Best Valid Loss: 0.3109\n",
      "Epoch 40 Avg Train Loss: 0.3011 Avg Valid Loss: 0.3314 / \n",
      "Epoch 41 Avg Train Loss: 0.2979 Avg Valid Loss: 0.3263 / \n",
      "Epoch 42 Avg Train Loss: 0.2965 Avg Valid Loss: 0.3302 / \n",
      "Epoch 43 Avg Train Loss: 0.3010 Avg Valid Loss: 0.3144 / \n",
      "Epoch 44 Avg Train Loss: 0.3009 Avg Valid Loss: 0.3222 / \n",
      "Epoch 45 Avg Train Loss: 0.2957 Avg Valid Loss: 0.3274 / \n",
      "Epoch 46 Avg Train Loss: 0.2898 Avg Valid Loss: 0.3379 / \n",
      "Epoch 47 Avg Train Loss: 0.2975 Avg Valid Loss: 0.3501 / \n",
      "Epoch 48 Avg Train Loss: 0.2954 Avg Valid Loss: 0.3325 / \n",
      "Epoch 49 Avg Train Loss: 0.2955 Avg Valid Loss: 0.3321 / \n",
      "Epoch 50 Avg Train Loss: 0.3030 Avg Valid Loss: 0.3279 / \n",
      "Epoch 51 Avg Train Loss: 0.2958 Avg Valid Loss: 0.3530 / \n",
      "Epoch 52 Avg Train Loss: 0.2946 Avg Valid Loss: 0.3284 / \n",
      "Epoch 53 Avg Train Loss: 0.2970 Avg Valid Loss: 0.3316 / \n",
      "Epoch 54 Avg Train Loss: 0.2989 Avg Valid Loss: 0.3400 / \n",
      "Epoch 55 Avg Train Loss: 0.2936 Avg Valid Loss: 0.3394 / \n",
      "Epoch 56 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3403 / \n",
      "Epoch 57 Avg Train Loss: 0.2979 Avg Valid Loss: 0.3283 / \n",
      "Epoch 58 Avg Train Loss: 0.2900 Avg Valid Loss: 0.3263 / \n",
      "Epoch 59 Avg Train Loss: 0.2987 Avg Valid Loss: 0.3262 / \n",
      "Epoch 60 Avg Train Loss: 0.2874 Avg Valid Loss: 0.3358 / \n",
      "Epoch 61 Avg Train Loss: 0.2922 Avg Valid Loss: 0.3192 / \n",
      "Epoch 62 Avg Train Loss: 0.2967 Avg Valid Loss: 0.3223 / \n",
      "Epoch 63 Avg Train Loss: 0.2888 Avg Valid Loss: 0.3220 / \n",
      "Epoch 64 Avg Train Loss: 0.2907 Avg Valid Loss: 0.3299 / \n",
      "Epoch 65 Avg Train Loss: 0.2929 Avg Valid Loss: 0.3457 / \n",
      "Epoch 66 Avg Train Loss: 0.2882 Avg Valid Loss: 0.3363 / \n",
      "Epoch 67 Avg Train Loss: 0.2946 Avg Valid Loss: 0.3409 / \n",
      "Epoch 68 Avg Train Loss: 0.2963 Avg Valid Loss: 0.3305 / \n",
      "Epoch 69 Avg Train Loss: 0.2795 Avg Valid Loss: 0.3510 / \n",
      "Epoch 70 Avg Train Loss: 0.2962 Avg Valid Loss: 0.3352 / \n",
      "Epoch 71 Avg Train Loss: 0.2858 Avg Valid Loss: 0.3393 / \n",
      "Epoch 72 Avg Train Loss: 0.2844 Avg Valid Loss: 0.3338 / \n",
      "Epoch 73 Avg Train Loss: 0.2897 Avg Valid Loss: 0.3256 / \n",
      "Epoch 74 Avg Train Loss: 0.2888 Avg Valid Loss: 0.3319 / \n",
      "Epoch 75 Avg Train Loss: 0.2858 Avg Valid Loss: 0.3219 / \n",
      "Epoch 76 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3267 / \n",
      "Epoch 77 Avg Train Loss: 0.2884 Avg Valid Loss: 0.3277 / \n",
      "Epoch 78 Avg Train Loss: 0.2920 Avg Valid Loss: 0.3210 / \n",
      "Epoch 79 Avg Train Loss: 0.2876 Avg Valid Loss: 0.3422 / \n",
      "Epoch 80 Avg Train Loss: 0.2842 Avg Valid Loss: 0.3140 / \n",
      "Epoch 81 Avg Train Loss: 0.2897 Avg Valid Loss: 0.3228 / \n",
      "Epoch 82 Avg Train Loss: 0.2889 Avg Valid Loss: 0.3173 / \n",
      "Epoch 83 Avg Train Loss: 0.2867 Avg Valid Loss: 0.3329 / \n",
      "Epoch 84 Avg Train Loss: 0.2919 Avg Valid Loss: 0.3287 / \n",
      "Epoch 85 Avg Train Loss: 0.2873 Avg Valid Loss: 0.3212 / \n",
      "Epoch 86 Avg Train Loss: 0.2842 Avg Valid Loss: 0.3337 / \n",
      "Epoch 87 Avg Train Loss: 0.2849 Avg Valid Loss: 0.3447 / \n",
      "Epoch 88 Avg Train Loss: 0.2874 Avg Valid Loss: 0.3665 / \n",
      "Epoch 89 Avg Train Loss: 0.2947 Avg Valid Loss: 0.3687 / \n",
      "Epoch 90 Avg Train Loss: 0.2942 Avg Valid Loss: 0.3300 / \n",
      "Epoch 91 Avg Train Loss: 0.2872 Avg Valid Loss: 0.3450 / \n",
      "Epoch 92 Avg Train Loss: 0.2829 Avg Valid Loss: 0.3167 / \n",
      "Epoch 93 Avg Train Loss: 0.2798 Avg Valid Loss: 0.3335 / \n",
      "Epoch 94 Avg Train Loss: 0.2804 Avg Valid Loss: 0.3233 / \n",
      "Epoch 95 Avg Train Loss: 0.2850 Avg Valid Loss: 0.3478 / \n",
      "Epoch 96 Avg Train Loss: 0.2797 Avg Valid Loss: 0.3300 / \n",
      "Epoch 97 Avg Train Loss: 0.2852 Avg Valid Loss: 0.3351 / \n",
      "Epoch 98 Avg Train Loss: 0.2869 Avg Valid Loss: 0.3299 / \n",
      "Epoch 99 Avg Train Loss: 0.2871 Avg Valid Loss: 0.3268 / \n",
      "Epoch 100 Avg Train Loss: 0.2866 Avg Valid Loss: 0.3266 / \n",
      "========== fold: 4 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3109\n",
      "============ CV score with best loss weights ============\n",
      "Stage 0: 0.3639\n",
      "============ CV score with best loss weights ============\n",
      "Stage 1: 0.3030\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_folds:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    stages_scores = {i: [] for i in CFG.train_stages}\n",
    "    stages_oof_df = {i: pd.DataFrame() for i in CFG.train_stages}\n",
    "\n",
    "    for fold in CFG.train_folds:\n",
    "\n",
    "        prev_dir = \"\"\n",
    "        for stage in range(len(CFG.total_evaluators)):\n",
    "\n",
    "            pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "            if not os.path.exists(pop_dir):\n",
    "                os.makedirs(pop_dir)\n",
    "\n",
    "            if stage not in CFG.train_stages:\n",
    "                prev_dir = pop_dir\n",
    "                continue\n",
    "\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            stages_oof_df[stage] = pd.concat([stages_oof_df[stage], train_oof_df])\n",
    "            stages_scores[stage].append(score)\n",
    "\n",
    "            prev_dir = pop_dir\n",
    "\n",
    "            LOGGER.info(f\"========== fold: {fold} stage: {stage+1} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "    for stage, scores in stages_scores.items():\n",
    "        LOGGER.info(f\"============ CV score with best loss weights ============\")\n",
    "        LOGGER.info(f\"Stage {stage}: {np.mean(scores):.4f}\")\n",
    "\n",
    "    for stage, oof_df in stages_oof_df.items():\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score with resnet1D_gru Raw EEG = 0.3030\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import kl_div\n",
    "\n",
    "# === Pre-process OOF ===\n",
    "gt = oof_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "gt.reset_index(inplace=True, drop=True)\n",
    "\n",
    "preds = oof_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "preds.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_trues = gt[CFG.target_cols]\n",
    "y_preds = preds[CFG.target_cols]\n",
    "\n",
    "oof = pd.DataFrame(y_preds.copy())\n",
    "oof[\"id\"] = np.arange(len(oof))\n",
    "\n",
    "true = pd.DataFrame(y_trues.copy())\n",
    "true[\"id\"] = np.arange(len(true))\n",
    "\n",
    "cv = kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "print(f\"CV Score with resnet1D_gru Raw EEG = {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
